{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ezamorano12/intro_deep_learning/blob/main/Eva_Zamorano_pruebas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "prospective-america",
      "metadata": {
        "id": "prospective-america"
      },
      "source": [
        "Dado que el entrenamiento de redes neuronales es una tarea  muy costosa, **se recomienda ejecutar el notebooks en [Google Colab](https://colab.research.google.com)**, por supuesto también se puede ejecutar en local.\n",
        "\n",
        "Al entrar en [Google Colab](https://colab.research.google.com) bastará con hacer click en `upload` y subir este notebook. No olvide luego descargarlo en `File->Download .ipynb`\n",
        "\n",
        "**El examen deberá ser entregado con las celdas ejecutadas, si alguna celda no está ejecutadas no se contará.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "novel-stewart",
      "metadata": {
        "id": "novel-stewart"
      },
      "source": [
        "El examen se divide en tres partes, con la puntuación que se indica a continuación. La puntuación máxima será 10.\n",
        "\n",
        "- [Actividad 1: Redes Densas](#actividad_1): 5.5 pts\n",
        "    - Correcta normalización: máximo de 0.5 pts\n",
        "    - [Cuestión 1](#1.1): 1 pt\n",
        "    - [Cuestión 2](#1.2): 1 pt\n",
        "    - [Cuestión 3](#1.3): 0.5 pts\n",
        "    - [Cuestión 4](#1.4): 0.5 pts\n",
        "    - [Cuestión 5](#1.5): 0.5 pts\n",
        "    - [Cuestión 6](#1.6): 0.5 pts\n",
        "    - [Cuestión 7](#1.7): 0.5 pts\n",
        "    - [Cuestión 8](#1.8): 0.5 pts\n",
        "\n",
        "\n",
        "- [Actividad 2: Redes Convolucionales](#actividad_2): 4.5 pts\n",
        "    - [Cuestión 1](#2.1): 1 pt\n",
        "    - [Cuestión 2](#2.2): 1.5 pt\n",
        "    - [Cuestión 3](#2.3): 0.5 pts\n",
        "    - [Cuestión 4](#2.4): 0.5 pts\n",
        "    - [Cuestión 5](#2.5): 0.5 pts\n",
        "    - [Cuestión 6](#2.6): 0.5 pts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 234,
      "id": "prompt-developer",
      "metadata": {
        "id": "prompt-developer"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vocal-correction",
      "metadata": {
        "id": "vocal-correction"
      },
      "source": [
        "<a name='actividad_1'></a>\n",
        "# Actividad 1: Redes Densas\n",
        "\n",
        "Para esta primera actividad vamos a utilizar el [boston housing dataset](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html). Con el que trataremos de predecir el precio de una casa con 13 features.\n",
        "\n",
        "**Puntuación**: \n",
        "\n",
        "Normalizar las features correctamente (x_train, x_test): 0.5 pts \n",
        "\n",
        "- Correcta normalización: máximo de 0.5 pts\n",
        "- [Cuestión 1](#1.1): 1 pt\n",
        "- [Cuestión 2](#1.2): 1 pt\n",
        "- [Cuestión 3](#1.3): 0.5 pts\n",
        "- [Cuestión 4](#1.4): 0.5 pts\n",
        "- [Cuestión 5](#1.5): 0.5 pts\n",
        "- [Cuestión 6](#1.6): 0.5 pts\n",
        "- [Cuestión 7](#1.7): 0.5 pts\n",
        "- [Cuestión 8](#1.8): 0.5 pts\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 315,
      "id": "presidential-milan",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "presidential-milan",
        "outputId": "baa21eeb-c050-4bbc-806d-79004ba73a09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train, y_train shapes: (404, 13) (404,)\n",
            "x_test, y_test shapes: (404, 13) (404,)\n",
            "Some prices:  [15.2 42.3 50.  21.1 17.7]\n"
          ]
        }
      ],
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.boston_housing.load_data(\n",
        "    path='boston_housing.npz',\n",
        "    test_split=0.2,\n",
        ")\n",
        "print('x_train, y_train shapes:', x_train.shape, y_train.shape)\n",
        "print('x_test, y_test shapes:', x_train.shape, y_train.shape)\n",
        "print('Some prices: ', y_train[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 316,
      "id": "painted-extreme",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "painted-extreme",
        "outputId": "83f2dbe2-412e-4e79-f992-3849e66bab48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train, y_train shapes: (404, 13) (404,)\n",
            "x_test, y_test shapes: (404, 13) (404,)\n",
            "Some prices:  [15.2 42.3 50.  21.1 17.7]\n"
          ]
        }
      ],
      "source": [
        "## Si quiere, puede normalizar las features\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.transform(x_test)\n",
        "print('x_train, y_train shapes:', x_train.shape, y_train.shape)\n",
        "print('x_test, y_test shapes:', x_train.shape, y_train.shape)\n",
        "print('Some prices: ', y_train[:5])\n",
        "#print('x_train mu, sigma', x_train_norm.mean(0), x_train_norm.std(0))\n",
        "#print('x_test mu, sigma', x_test_norm.mean(0), x_test_norm.std(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "underlying-planner",
      "metadata": {
        "id": "underlying-planner"
      },
      "source": [
        "<a name='1.1'></a>\n",
        "## Cuestión 1: Cree un modelo secuencial que contenga 4 capas ocultas(hidden layers), con más de 60 neuronas  por capa, sin regularización y obtenga los resultados.\n",
        "\n",
        "Puntuación: \n",
        "- Obtener el modelo correcto: 0.8 pts\n",
        "- Compilar el modelo: 0.1pts\n",
        "- Acertar con la función de pérdida: 0.1 pts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 415,
      "id": "working-shade",
      "metadata": {
        "id": "working-shade"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "# Código aquí\n",
        "model.add(layers.Dense(1024,input_shape=(13,),activation='relu'))\n",
        "model.add(layers.Dense(512,activation='relu'))\n",
        "model.add(layers.Dense(512,activation='relu'))\n",
        "model.add(layers.Dense(64,activation='relu'))\n",
        "model.add(layers.Dense(1,activation='relu'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 416,
      "id": "mobile-change",
      "metadata": {
        "id": "mobile-change"
      },
      "outputs": [],
      "source": [
        "# Compilación del modelo\n",
        "# Código aquí\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mse'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 417,
      "id": "rotary-credits",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rotary-credits",
        "outputId": "07fb4aa9-da1c-45f8-dcbf-a532eae9c15e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "11/11 [==============================] - 1s 32ms/step - loss: 345.2735 - mse: 345.2735 - val_loss: 116.6285 - val_mse: 116.6285\n",
            "Epoch 2/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 83.0892 - mse: 83.0892 - val_loss: 41.1165 - val_mse: 41.1165\n",
            "Epoch 3/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 38.2901 - mse: 38.2901 - val_loss: 30.3845 - val_mse: 30.3845\n",
            "Epoch 4/200\n",
            "11/11 [==============================] - 0s 19ms/step - loss: 25.1229 - mse: 25.1229 - val_loss: 22.0785 - val_mse: 22.0785\n",
            "Epoch 5/200\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 17.4853 - mse: 17.4853 - val_loss: 17.8692 - val_mse: 17.8692\n",
            "Epoch 6/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 14.6336 - mse: 14.6336 - val_loss: 16.7795 - val_mse: 16.7795\n",
            "Epoch 7/200\n",
            "11/11 [==============================] - 0s 19ms/step - loss: 12.7739 - mse: 12.7739 - val_loss: 15.7829 - val_mse: 15.7829\n",
            "Epoch 8/200\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 11.8041 - mse: 11.8041 - val_loss: 15.5770 - val_mse: 15.5770\n",
            "Epoch 9/200\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 12.1513 - mse: 12.1513 - val_loss: 15.6621 - val_mse: 15.6621\n",
            "Epoch 10/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 10.8960 - mse: 10.8960 - val_loss: 14.4476 - val_mse: 14.4476\n",
            "Epoch 11/200\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 11.4392 - mse: 11.4392 - val_loss: 18.4328 - val_mse: 18.4328\n",
            "Epoch 12/200\n",
            "11/11 [==============================] - 0s 19ms/step - loss: 11.4462 - mse: 11.4462 - val_loss: 14.2046 - val_mse: 14.2046\n",
            "Epoch 13/200\n",
            "11/11 [==============================] - 0s 19ms/step - loss: 10.2475 - mse: 10.2475 - val_loss: 13.5208 - val_mse: 13.5208\n",
            "Epoch 14/200\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 8.8215 - mse: 8.8215 - val_loss: 13.4985 - val_mse: 13.4985\n",
            "Epoch 15/200\n",
            "11/11 [==============================] - 0s 16ms/step - loss: 8.5994 - mse: 8.5994 - val_loss: 18.3705 - val_mse: 18.3705\n",
            "Epoch 16/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 18.3543 - mse: 18.3543 - val_loss: 14.9471 - val_mse: 14.9471\n",
            "Epoch 17/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 14.3246 - mse: 14.3246 - val_loss: 22.9949 - val_mse: 22.9949\n",
            "Epoch 18/200\n",
            "11/11 [==============================] - 0s 19ms/step - loss: 10.8948 - mse: 10.8948 - val_loss: 20.6727 - val_mse: 20.6727\n",
            "Epoch 19/200\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 9.7632 - mse: 9.7632 - val_loss: 16.5233 - val_mse: 16.5233\n",
            "Epoch 20/200\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 8.8691 - mse: 8.8691 - val_loss: 12.6870 - val_mse: 12.6870\n",
            "Epoch 21/200\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 7.6469 - mse: 7.6469 - val_loss: 14.4866 - val_mse: 14.4866\n",
            "Epoch 22/200\n",
            "11/11 [==============================] - 0s 19ms/step - loss: 7.3391 - mse: 7.3391 - val_loss: 13.9179 - val_mse: 13.9179\n",
            "Epoch 23/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 6.5966 - mse: 6.5966 - val_loss: 14.9853 - val_mse: 14.9853\n",
            "Epoch 24/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 7.3114 - mse: 7.3114 - val_loss: 14.8055 - val_mse: 14.8055\n",
            "Epoch 25/200\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 10.1239 - mse: 10.1239 - val_loss: 19.2377 - val_mse: 19.2377\n",
            "Epoch 26/200\n",
            "11/11 [==============================] - 0s 19ms/step - loss: 8.1621 - mse: 8.1621 - val_loss: 15.6033 - val_mse: 15.6033\n",
            "Epoch 27/200\n",
            "11/11 [==============================] - 0s 19ms/step - loss: 6.3833 - mse: 6.3833 - val_loss: 14.2813 - val_mse: 14.2813\n",
            "Epoch 28/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 7.1351 - mse: 7.1351 - val_loss: 11.8054 - val_mse: 11.8054\n",
            "Epoch 29/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 5.9750 - mse: 5.9750 - val_loss: 13.4247 - val_mse: 13.4247\n",
            "Epoch 30/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 5.6774 - mse: 5.6774 - val_loss: 14.1689 - val_mse: 14.1689\n",
            "Epoch 31/200\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 7.8863 - mse: 7.8863 - val_loss: 16.9659 - val_mse: 16.9659\n",
            "Epoch 32/200\n",
            "11/11 [==============================] - 0s 20ms/step - loss: 6.7515 - mse: 6.7515 - val_loss: 16.7183 - val_mse: 16.7183\n",
            "Epoch 33/200\n",
            "11/11 [==============================] - 0s 19ms/step - loss: 4.7850 - mse: 4.7850 - val_loss: 14.0500 - val_mse: 14.0500\n",
            "Epoch 34/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 5.2531 - mse: 5.2531 - val_loss: 12.6895 - val_mse: 12.6895\n",
            "Epoch 35/200\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 4.5507 - mse: 4.5507 - val_loss: 15.6141 - val_mse: 15.6141\n",
            "Epoch 36/200\n",
            "11/11 [==============================] - 0s 20ms/step - loss: 4.4986 - mse: 4.4986 - val_loss: 12.3657 - val_mse: 12.3657\n",
            "Epoch 37/200\n",
            "11/11 [==============================] - 0s 19ms/step - loss: 4.8661 - mse: 4.8661 - val_loss: 12.2557 - val_mse: 12.2557\n",
            "Epoch 38/200\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 4.1950 - mse: 4.1950 - val_loss: 13.2690 - val_mse: 13.2690\n",
            "Epoch 39/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 4.1809 - mse: 4.1809 - val_loss: 12.7223 - val_mse: 12.7223\n",
            "Epoch 40/200\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 4.9185 - mse: 4.9185 - val_loss: 12.9021 - val_mse: 12.9021\n",
            "Epoch 41/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 5.1204 - mse: 5.1204 - val_loss: 11.8571 - val_mse: 11.8571\n",
            "Epoch 42/200\n",
            "11/11 [==============================] - 0s 19ms/step - loss: 4.6889 - mse: 4.6889 - val_loss: 14.9170 - val_mse: 14.9170\n",
            "Epoch 43/200\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 4.5755 - mse: 4.5755 - val_loss: 12.7048 - val_mse: 12.7048\n",
            "Epoch 44/200\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 3.6394 - mse: 3.6394 - val_loss: 11.1281 - val_mse: 11.1281\n",
            "Epoch 45/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 3.7622 - mse: 3.7622 - val_loss: 12.1685 - val_mse: 12.1685\n",
            "Epoch 46/200\n",
            "11/11 [==============================] - 0s 20ms/step - loss: 6.3800 - mse: 6.3800 - val_loss: 23.0810 - val_mse: 23.0810\n",
            "Epoch 47/200\n",
            "11/11 [==============================] - 0s 20ms/step - loss: 10.3442 - mse: 10.3442 - val_loss: 14.7450 - val_mse: 14.7450\n",
            "Epoch 48/200\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 5.0476 - mse: 5.0476 - val_loss: 9.9785 - val_mse: 9.9785\n",
            "Epoch 49/200\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 4.4772 - mse: 4.4772 - val_loss: 16.1636 - val_mse: 16.1636\n",
            "Epoch 50/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 5.5052 - mse: 5.5052 - val_loss: 11.0509 - val_mse: 11.0509\n",
            "Epoch 51/200\n",
            "11/11 [==============================] - 0s 19ms/step - loss: 3.5893 - mse: 3.5893 - val_loss: 11.9687 - val_mse: 11.9687\n",
            "Epoch 52/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 3.5901 - mse: 3.5901 - val_loss: 15.7383 - val_mse: 15.7383\n",
            "Epoch 53/200\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 3.3410 - mse: 3.3410 - val_loss: 13.4358 - val_mse: 13.4358\n",
            "Epoch 54/200\n",
            "11/11 [==============================] - 0s 19ms/step - loss: 3.8222 - mse: 3.8222 - val_loss: 17.2881 - val_mse: 17.2881\n",
            "Epoch 55/200\n",
            "11/11 [==============================] - 0s 19ms/step - loss: 4.1955 - mse: 4.1955 - val_loss: 12.1936 - val_mse: 12.1936\n",
            "Epoch 56/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 3.7646 - mse: 3.7646 - val_loss: 12.4991 - val_mse: 12.4991\n",
            "Epoch 57/200\n",
            "11/11 [==============================] - 0s 19ms/step - loss: 4.0024 - mse: 4.0024 - val_loss: 18.3907 - val_mse: 18.3907\n",
            "Epoch 58/200\n",
            "11/11 [==============================] - 0s 19ms/step - loss: 5.1882 - mse: 5.1882 - val_loss: 13.8522 - val_mse: 13.8522\n",
            "Epoch 59/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 4.5387 - mse: 4.5387 - val_loss: 12.5648 - val_mse: 12.5648\n",
            "Epoch 60/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 3.6030 - mse: 3.6030 - val_loss: 10.9228 - val_mse: 10.9228\n",
            "Epoch 61/200\n",
            "11/11 [==============================] - 0s 19ms/step - loss: 2.9312 - mse: 2.9312 - val_loss: 11.5120 - val_mse: 11.5120\n",
            "Epoch 62/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 3.6888 - mse: 3.6888 - val_loss: 11.8288 - val_mse: 11.8288\n",
            "Epoch 63/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 3.0683 - mse: 3.0683 - val_loss: 11.9050 - val_mse: 11.9050\n",
            "Epoch 64/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 3.5461 - mse: 3.5461 - val_loss: 12.3818 - val_mse: 12.3818\n",
            "Epoch 65/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 4.3171 - mse: 4.3171 - val_loss: 10.7171 - val_mse: 10.7171\n",
            "Epoch 66/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 4.1131 - mse: 4.1131 - val_loss: 11.3047 - val_mse: 11.3047\n",
            "Epoch 67/200\n",
            "11/11 [==============================] - 0s 20ms/step - loss: 4.0047 - mse: 4.0047 - val_loss: 16.0146 - val_mse: 16.0146\n",
            "Epoch 68/200\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 3.6715 - mse: 3.6715 - val_loss: 14.2635 - val_mse: 14.2635\n",
            "Epoch 69/200\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 4.4323 - mse: 4.4323 - val_loss: 11.4031 - val_mse: 11.4031\n",
            "Epoch 70/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 4.3921 - mse: 4.3921 - val_loss: 10.9778 - val_mse: 10.9778\n",
            "Epoch 71/200\n",
            "11/11 [==============================] - 0s 19ms/step - loss: 3.3777 - mse: 3.3777 - val_loss: 10.1657 - val_mse: 10.1657\n",
            "Epoch 72/200\n",
            "11/11 [==============================] - 0s 20ms/step - loss: 2.8329 - mse: 2.8329 - val_loss: 10.6114 - val_mse: 10.6114\n",
            "Epoch 73/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 2.9329 - mse: 2.9329 - val_loss: 11.9335 - val_mse: 11.9335\n",
            "Epoch 74/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 2.4876 - mse: 2.4876 - val_loss: 11.0769 - val_mse: 11.0769\n",
            "Epoch 75/200\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 3.2067 - mse: 3.2067 - val_loss: 13.6655 - val_mse: 13.6655\n",
            "Epoch 76/200\n",
            "11/11 [==============================] - 0s 19ms/step - loss: 4.3416 - mse: 4.3416 - val_loss: 13.9453 - val_mse: 13.9453\n",
            "Epoch 77/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 4.0602 - mse: 4.0602 - val_loss: 10.2144 - val_mse: 10.2144\n",
            "Epoch 78/200\n",
            "11/11 [==============================] - 0s 19ms/step - loss: 3.9360 - mse: 3.9360 - val_loss: 10.0518 - val_mse: 10.0518\n",
            "Epoch 79/200\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 3.5280 - mse: 3.5280 - val_loss: 11.5170 - val_mse: 11.5170\n",
            "Epoch 80/200\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 2.8216 - mse: 2.8216 - val_loss: 10.2658 - val_mse: 10.2658\n",
            "Epoch 81/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 2.1843 - mse: 2.1843 - val_loss: 10.7492 - val_mse: 10.7492\n",
            "Epoch 82/200\n",
            "11/11 [==============================] - 0s 20ms/step - loss: 2.1255 - mse: 2.1255 - val_loss: 10.7390 - val_mse: 10.7390\n",
            "Epoch 83/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 2.0491 - mse: 2.0491 - val_loss: 10.4946 - val_mse: 10.4946\n",
            "Epoch 84/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 2.4746 - mse: 2.4746 - val_loss: 11.1662 - val_mse: 11.1662\n",
            "Epoch 85/200\n",
            "11/11 [==============================] - 0s 25ms/step - loss: 2.0283 - mse: 2.0283 - val_loss: 10.0967 - val_mse: 10.0967\n",
            "Epoch 86/200\n",
            "11/11 [==============================] - 0s 26ms/step - loss: 2.2784 - mse: 2.2784 - val_loss: 11.6065 - val_mse: 11.6065\n",
            "Epoch 87/200\n",
            "11/11 [==============================] - 0s 27ms/step - loss: 2.6821 - mse: 2.6821 - val_loss: 10.7708 - val_mse: 10.7708\n",
            "Epoch 88/200\n",
            "11/11 [==============================] - 0s 25ms/step - loss: 1.9740 - mse: 1.9740 - val_loss: 11.4891 - val_mse: 11.4891\n",
            "Epoch 89/200\n",
            "11/11 [==============================] - 0s 26ms/step - loss: 2.8249 - mse: 2.8249 - val_loss: 10.3100 - val_mse: 10.3100\n",
            "Epoch 90/200\n",
            "11/11 [==============================] - 0s 27ms/step - loss: 2.4441 - mse: 2.4441 - val_loss: 13.3888 - val_mse: 13.3888\n",
            "Epoch 91/200\n",
            "11/11 [==============================] - 0s 25ms/step - loss: 2.6760 - mse: 2.6760 - val_loss: 12.9404 - val_mse: 12.9404\n",
            "Epoch 92/200\n",
            "11/11 [==============================] - 0s 28ms/step - loss: 2.4142 - mse: 2.4142 - val_loss: 12.0652 - val_mse: 12.0652\n",
            "Epoch 93/200\n",
            "11/11 [==============================] - 0s 21ms/step - loss: 2.9205 - mse: 2.9205 - val_loss: 11.4817 - val_mse: 11.4817\n",
            "Epoch 94/200\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 2.1391 - mse: 2.1391 - val_loss: 12.7754 - val_mse: 12.7754\n",
            "Epoch 95/200\n",
            "11/11 [==============================] - 0s 19ms/step - loss: 2.5391 - mse: 2.5391 - val_loss: 11.5532 - val_mse: 11.5532\n",
            "Epoch 96/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 1.8342 - mse: 1.8342 - val_loss: 10.3562 - val_mse: 10.3562\n",
            "Epoch 97/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 1.8617 - mse: 1.8617 - val_loss: 13.2718 - val_mse: 13.2718\n",
            "Epoch 98/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 2.0698 - mse: 2.0698 - val_loss: 11.0953 - val_mse: 11.0953\n",
            "Epoch 99/200\n",
            "11/11 [==============================] - 0s 19ms/step - loss: 2.2201 - mse: 2.2201 - val_loss: 10.0009 - val_mse: 10.0009\n",
            "Epoch 100/200\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 1.7161 - mse: 1.7161 - val_loss: 11.3876 - val_mse: 11.3876\n",
            "Epoch 101/200\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 1.8686 - mse: 1.8686 - val_loss: 10.7755 - val_mse: 10.7755\n",
            "Epoch 102/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 1.7661 - mse: 1.7661 - val_loss: 10.5657 - val_mse: 10.5657\n",
            "Epoch 103/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 2.5422 - mse: 2.5422 - val_loss: 12.2563 - val_mse: 12.2563\n",
            "Epoch 104/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 3.9315 - mse: 3.9315 - val_loss: 13.5336 - val_mse: 13.5336\n",
            "Epoch 105/200\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 7.4318 - mse: 7.4318 - val_loss: 11.6178 - val_mse: 11.6178\n",
            "Epoch 106/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 11.1282 - mse: 11.1282 - val_loss: 13.6610 - val_mse: 13.6610\n",
            "Epoch 107/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 6.1992 - mse: 6.1992 - val_loss: 12.4154 - val_mse: 12.4154\n",
            "Epoch 108/200\n",
            "11/11 [==============================] - 0s 19ms/step - loss: 4.7127 - mse: 4.7127 - val_loss: 11.4520 - val_mse: 11.4520\n",
            "Epoch 109/200\n",
            "11/11 [==============================] - 0s 19ms/step - loss: 4.6488 - mse: 4.6488 - val_loss: 9.8573 - val_mse: 9.8573\n",
            "Epoch 110/200\n",
            "11/11 [==============================] - 0s 19ms/step - loss: 3.6425 - mse: 3.6425 - val_loss: 12.6404 - val_mse: 12.6404\n",
            "Epoch 111/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 4.1860 - mse: 4.1860 - val_loss: 9.4980 - val_mse: 9.4980\n",
            "Epoch 112/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 3.1816 - mse: 3.1816 - val_loss: 9.7659 - val_mse: 9.7659\n",
            "Epoch 113/200\n",
            "11/11 [==============================] - 0s 19ms/step - loss: 2.4393 - mse: 2.4393 - val_loss: 10.0020 - val_mse: 10.0020\n",
            "Epoch 114/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 2.2014 - mse: 2.2014 - val_loss: 11.9211 - val_mse: 11.9211\n",
            "Epoch 115/200\n",
            "11/11 [==============================] - 0s 19ms/step - loss: 2.7181 - mse: 2.7181 - val_loss: 13.3393 - val_mse: 13.3393\n",
            "Epoch 116/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 2.9535 - mse: 2.9535 - val_loss: 11.5549 - val_mse: 11.5549\n",
            "Epoch 117/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 3.3326 - mse: 3.3326 - val_loss: 11.5999 - val_mse: 11.5999\n",
            "Epoch 118/200\n",
            "11/11 [==============================] - 0s 19ms/step - loss: 5.6876 - mse: 5.6876 - val_loss: 14.6829 - val_mse: 14.6829\n",
            "Epoch 119/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 6.1750 - mse: 6.1750 - val_loss: 14.9959 - val_mse: 14.9959\n",
            "Epoch 120/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 6.1482 - mse: 6.1482 - val_loss: 12.1842 - val_mse: 12.1842\n",
            "Epoch 121/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 3.2515 - mse: 3.2515 - val_loss: 13.2364 - val_mse: 13.2364\n",
            "Epoch 122/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 2.3744 - mse: 2.3744 - val_loss: 10.5306 - val_mse: 10.5306\n",
            "Epoch 123/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 1.7843 - mse: 1.7843 - val_loss: 10.0317 - val_mse: 10.0317\n",
            "Epoch 124/200\n",
            "11/11 [==============================] - 0s 20ms/step - loss: 1.7912 - mse: 1.7912 - val_loss: 10.9657 - val_mse: 10.9657\n",
            "Epoch 125/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 1.9412 - mse: 1.9412 - val_loss: 10.5199 - val_mse: 10.5199\n",
            "Epoch 126/200\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 1.8156 - mse: 1.8156 - val_loss: 10.1601 - val_mse: 10.1601\n",
            "Epoch 127/200\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 1.5898 - mse: 1.5898 - val_loss: 10.1963 - val_mse: 10.1963\n",
            "Epoch 128/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 2.1367 - mse: 2.1367 - val_loss: 10.9944 - val_mse: 10.9944\n",
            "Epoch 129/200\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 2.0483 - mse: 2.0483 - val_loss: 11.4277 - val_mse: 11.4277\n",
            "Epoch 130/200\n",
            "11/11 [==============================] - 0s 19ms/step - loss: 1.5624 - mse: 1.5624 - val_loss: 11.6213 - val_mse: 11.6213\n",
            "Epoch 131/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 1.6596 - mse: 1.6596 - val_loss: 10.8665 - val_mse: 10.8665\n",
            "Epoch 132/200\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 1.8305 - mse: 1.8305 - val_loss: 11.0946 - val_mse: 11.0946\n",
            "Epoch 133/200\n",
            "11/11 [==============================] - 0s 20ms/step - loss: 1.7689 - mse: 1.7689 - val_loss: 13.0014 - val_mse: 13.0014\n",
            "Epoch 134/200\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 1.6605 - mse: 1.6605 - val_loss: 11.3263 - val_mse: 11.3263\n",
            "Epoch 135/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 1.8429 - mse: 1.8429 - val_loss: 10.7257 - val_mse: 10.7257\n",
            "Epoch 136/200\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 1.5966 - mse: 1.5966 - val_loss: 11.0097 - val_mse: 11.0097\n",
            "Epoch 137/200\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 1.5303 - mse: 1.5303 - val_loss: 11.1379 - val_mse: 11.1379\n",
            "Epoch 138/200\n",
            "11/11 [==============================] - 0s 20ms/step - loss: 1.5056 - mse: 1.5056 - val_loss: 11.6219 - val_mse: 11.6219\n",
            "Epoch 139/200\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 1.3316 - mse: 1.3316 - val_loss: 10.4866 - val_mse: 10.4866\n",
            "Epoch 140/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 1.5623 - mse: 1.5623 - val_loss: 10.7438 - val_mse: 10.7438\n",
            "Epoch 141/200\n",
            "11/11 [==============================] - 0s 19ms/step - loss: 1.5339 - mse: 1.5339 - val_loss: 10.1449 - val_mse: 10.1449\n",
            "Epoch 142/200\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 1.3485 - mse: 1.3485 - val_loss: 14.6121 - val_mse: 14.6121\n",
            "Epoch 143/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 1.8918 - mse: 1.8918 - val_loss: 12.0888 - val_mse: 12.0888\n",
            "Epoch 144/200\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 2.1497 - mse: 2.1497 - val_loss: 9.5484 - val_mse: 9.5484\n",
            "Epoch 145/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 1.7518 - mse: 1.7518 - val_loss: 10.8082 - val_mse: 10.8082\n",
            "Epoch 146/200\n",
            "11/11 [==============================] - 0s 19ms/step - loss: 1.5798 - mse: 1.5798 - val_loss: 11.3321 - val_mse: 11.3321\n",
            "Epoch 147/200\n",
            "11/11 [==============================] - 0s 19ms/step - loss: 1.6425 - mse: 1.6425 - val_loss: 11.5963 - val_mse: 11.5963\n",
            "Epoch 148/200\n",
            "11/11 [==============================] - 0s 19ms/step - loss: 1.6553 - mse: 1.6553 - val_loss: 11.4162 - val_mse: 11.4162\n",
            "Epoch 149/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 1.3378 - mse: 1.3378 - val_loss: 12.5700 - val_mse: 12.5700\n",
            "Epoch 150/200\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 1.5064 - mse: 1.5064 - val_loss: 10.6858 - val_mse: 10.6858\n",
            "Epoch 151/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 1.9566 - mse: 1.9566 - val_loss: 15.8027 - val_mse: 15.8027\n",
            "Epoch 152/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 4.7556 - mse: 4.7556 - val_loss: 14.6020 - val_mse: 14.6020\n",
            "Epoch 153/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 3.6209 - mse: 3.6209 - val_loss: 12.5033 - val_mse: 12.5033\n",
            "Epoch 154/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 3.6275 - mse: 3.6275 - val_loss: 12.8470 - val_mse: 12.8470\n",
            "Epoch 155/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 2.6185 - mse: 2.6185 - val_loss: 12.2256 - val_mse: 12.2256\n",
            "Epoch 156/200\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 2.0376 - mse: 2.0376 - val_loss: 11.5117 - val_mse: 11.5117\n",
            "Epoch 157/200\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 1.8389 - mse: 1.8389 - val_loss: 11.5688 - val_mse: 11.5688\n",
            "Epoch 158/200\n",
            "11/11 [==============================] - 0s 19ms/step - loss: 2.1093 - mse: 2.1093 - val_loss: 11.2769 - val_mse: 11.2769\n",
            "Epoch 159/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 1.5215 - mse: 1.5215 - val_loss: 12.1194 - val_mse: 12.1194\n",
            "Epoch 160/200\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 1.7272 - mse: 1.7272 - val_loss: 11.7047 - val_mse: 11.7047\n",
            "Epoch 161/200\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 2.0579 - mse: 2.0579 - val_loss: 10.7573 - val_mse: 10.7573\n",
            "Epoch 162/200\n",
            "11/11 [==============================] - 0s 19ms/step - loss: 1.6743 - mse: 1.6743 - val_loss: 15.4631 - val_mse: 15.4631\n",
            "Epoch 163/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 1.8172 - mse: 1.8172 - val_loss: 12.0725 - val_mse: 12.0725\n",
            "Epoch 164/200\n",
            "11/11 [==============================] - 0s 19ms/step - loss: 1.3902 - mse: 1.3902 - val_loss: 11.7305 - val_mse: 11.7305\n",
            "Epoch 165/200\n",
            "11/11 [==============================] - 0s 21ms/step - loss: 1.3901 - mse: 1.3901 - val_loss: 10.0836 - val_mse: 10.0836\n",
            "Epoch 166/200\n",
            "11/11 [==============================] - 0s 19ms/step - loss: 1.4667 - mse: 1.4667 - val_loss: 13.3958 - val_mse: 13.3958\n",
            "Epoch 167/200\n",
            "11/11 [==============================] - 0s 19ms/step - loss: 1.3682 - mse: 1.3682 - val_loss: 12.4933 - val_mse: 12.4933\n",
            "Epoch 168/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 1.6460 - mse: 1.6460 - val_loss: 11.6305 - val_mse: 11.6305\n",
            "Epoch 169/200\n",
            "11/11 [==============================] - 0s 20ms/step - loss: 1.7307 - mse: 1.7307 - val_loss: 12.5696 - val_mse: 12.5696\n",
            "Epoch 170/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 1.3419 - mse: 1.3419 - val_loss: 11.2195 - val_mse: 11.2195\n",
            "Epoch 171/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 1.1039 - mse: 1.1039 - val_loss: 11.2502 - val_mse: 11.2502\n",
            "Epoch 172/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 1.0618 - mse: 1.0618 - val_loss: 11.3973 - val_mse: 11.3973\n",
            "Epoch 173/200\n",
            "11/11 [==============================] - 0s 19ms/step - loss: 1.3040 - mse: 1.3040 - val_loss: 10.3107 - val_mse: 10.3107\n",
            "Epoch 174/200\n",
            "11/11 [==============================] - 0s 19ms/step - loss: 3.4912 - mse: 3.4912 - val_loss: 11.8288 - val_mse: 11.8288\n",
            "Epoch 175/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 2.4511 - mse: 2.4511 - val_loss: 15.5459 - val_mse: 15.5459\n",
            "Epoch 176/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 1.6308 - mse: 1.6308 - val_loss: 13.6860 - val_mse: 13.6860\n",
            "Epoch 177/200\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 2.2300 - mse: 2.2300 - val_loss: 13.9498 - val_mse: 13.9498\n",
            "Epoch 178/200\n",
            "11/11 [==============================] - 0s 20ms/step - loss: 2.1903 - mse: 2.1903 - val_loss: 12.6404 - val_mse: 12.6404\n",
            "Epoch 179/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 2.4953 - mse: 2.4953 - val_loss: 12.9821 - val_mse: 12.9821\n",
            "Epoch 180/200\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 1.4573 - mse: 1.4573 - val_loss: 12.2847 - val_mse: 12.2847\n",
            "Epoch 181/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 1.1590 - mse: 1.1590 - val_loss: 12.2685 - val_mse: 12.2685\n",
            "Epoch 182/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 1.3095 - mse: 1.3095 - val_loss: 12.4767 - val_mse: 12.4767\n",
            "Epoch 183/200\n",
            "11/11 [==============================] - 0s 20ms/step - loss: 2.1832 - mse: 2.1832 - val_loss: 12.5532 - val_mse: 12.5532\n",
            "Epoch 184/200\n",
            "11/11 [==============================] - 0s 19ms/step - loss: 1.2996 - mse: 1.2996 - val_loss: 11.1020 - val_mse: 11.1020\n",
            "Epoch 185/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 1.4023 - mse: 1.4023 - val_loss: 11.7652 - val_mse: 11.7652\n",
            "Epoch 186/200\n",
            "11/11 [==============================] - 0s 20ms/step - loss: 1.3632 - mse: 1.3632 - val_loss: 9.9728 - val_mse: 9.9728\n",
            "Epoch 187/200\n",
            "11/11 [==============================] - 0s 19ms/step - loss: 1.4460 - mse: 1.4460 - val_loss: 13.9777 - val_mse: 13.9777\n",
            "Epoch 188/200\n",
            "11/11 [==============================] - 0s 20ms/step - loss: 6.8855 - mse: 6.8855 - val_loss: 10.5718 - val_mse: 10.5718\n",
            "Epoch 189/200\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 5.6632 - mse: 5.6632 - val_loss: 12.3684 - val_mse: 12.3684\n",
            "Epoch 190/200\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 4.3167 - mse: 4.3167 - val_loss: 11.9007 - val_mse: 11.9007\n",
            "Epoch 191/200\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 3.5505 - mse: 3.5505 - val_loss: 14.6322 - val_mse: 14.6322\n",
            "Epoch 192/200\n",
            "11/11 [==============================] - 0s 19ms/step - loss: 3.2103 - mse: 3.2103 - val_loss: 14.2551 - val_mse: 14.2551\n",
            "Epoch 193/200\n",
            "11/11 [==============================] - 0s 19ms/step - loss: 2.2821 - mse: 2.2821 - val_loss: 10.3424 - val_mse: 10.3424\n",
            "Epoch 194/200\n",
            "11/11 [==============================] - 0s 19ms/step - loss: 1.6369 - mse: 1.6369 - val_loss: 12.9135 - val_mse: 12.9135\n",
            "Epoch 195/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 1.3040 - mse: 1.3040 - val_loss: 12.6106 - val_mse: 12.6106\n",
            "Epoch 196/200\n",
            "11/11 [==============================] - 0s 19ms/step - loss: 1.0891 - mse: 1.0891 - val_loss: 12.0507 - val_mse: 12.0507\n",
            "Epoch 197/200\n",
            "11/11 [==============================] - 0s 19ms/step - loss: 1.2174 - mse: 1.2174 - val_loss: 11.1849 - val_mse: 11.1849\n",
            "Epoch 198/200\n",
            "11/11 [==============================] - 0s 20ms/step - loss: 0.9696 - mse: 0.9696 - val_loss: 12.9298 - val_mse: 12.9298\n",
            "Epoch 199/200\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 1.2949 - mse: 1.2949 - val_loss: 10.7386 - val_mse: 10.7386\n",
            "Epoch 200/200\n",
            "11/11 [==============================] - 0s 19ms/step - loss: 1.1457 - mse: 1.1457 - val_loss: 12.8632 - val_mse: 12.8632\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fcaf1a48a50>"
            ]
          },
          "metadata": {},
          "execution_count": 417
        }
      ],
      "source": [
        "# No modifique el código\n",
        "model.fit(x_train,\n",
        "          y_train,\n",
        "          epochs=200,\n",
        "          batch_size=32,\n",
        "          validation_split=0.2,\n",
        "          verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 418,
      "id": "descending-letters",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "descending-letters",
        "outputId": "72a376fc-4347-4e2e-a9fe-8c452280b3f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 5ms/step - loss: 16.6041 - mse: 16.6041\n",
            "Test Loss: [16.604066848754883, 16.604066848754883]\n"
          ]
        }
      ],
      "source": [
        "# No modifique el código\n",
        "results = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test Loss: {}'.format(results))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "raised-delivery",
      "metadata": {
        "id": "raised-delivery"
      },
      "source": [
        "<a name='1.2'></a>\n",
        "## Cuestión 2: Utilice el mismo modelo de la cuestión anterior pero añadiendo al menos dos técnicas distinas de regularización.\n",
        "\n",
        "Ejemplos de regularización: [Prevent_Overfitting.ipynb](https://github.com/ezponda/intro_deep_learning/blob/main/class/Fundamentals/Prevent_Overfitting.ipynb)\n",
        "\n",
        "Puntuación:\n",
        "\n",
        "- Obtener el modelo con la regularización: 0.8 pts\n",
        "- Obtener un `test loss` inferior al anterior: 0.2 pts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 419,
      "id": "hired-ground",
      "metadata": {
        "id": "hired-ground"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "# Código aquí\n",
        "tf.keras.regularizers.l1(l1=0.01)\n",
        "\n",
        "model.add(layers.Dense(1024,input_shape=(13,),activation='relu'))\n",
        "model.add(layers.Dropout(0.3))\n",
        "model.add(layers.Dense(512,activation='relu'))\n",
        "model.add(layers.Dense(512,activation='relu'))\n",
        "model.add(layers.Dense(64,activation='relu',kernel_regularizer='l1'))\n",
        "model.add(layers.Dense(1,activation='relu'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 420,
      "id": "focal-traffic",
      "metadata": {
        "id": "focal-traffic"
      },
      "outputs": [],
      "source": [
        "# Compilación del modelo\n",
        "# Código aquí\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mse'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 421,
      "id": "338f8622",
      "metadata": {
        "id": "338f8622"
      },
      "outputs": [],
      "source": [
        "#reducimos el batch size como otro metodo de regulacion\n",
        "batch_size=16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 422,
      "id": "prostate-instrumentation",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prostate-instrumentation",
        "outputId": "ddc81b62-d6b7-497d-ec48-a4add6f1419e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "21/21 [==============================] - 1s 20ms/step - loss: 227.7730 - mse: 212.2702 - val_loss: 70.9349 - val_mse: 56.8660\n",
            "Epoch 2/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 45.9526 - mse: 33.0577 - val_loss: 33.2938 - val_mse: 21.7399\n",
            "Epoch 3/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 30.1958 - mse: 19.6403 - val_loss: 27.3597 - val_mse: 17.8942\n",
            "Epoch 4/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 25.4637 - mse: 16.7607 - val_loss: 22.6174 - val_mse: 14.7134\n",
            "Epoch 5/200\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 21.7072 - mse: 14.2941 - val_loss: 21.6350 - val_mse: 14.6849\n",
            "Epoch 6/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 20.5449 - mse: 13.9077 - val_loss: 25.2600 - val_mse: 18.9533\n",
            "Epoch 7/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 21.9095 - mse: 15.8370 - val_loss: 21.2490 - val_mse: 15.4155\n",
            "Epoch 8/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 17.0572 - mse: 11.4011 - val_loss: 20.9008 - val_mse: 15.4307\n",
            "Epoch 9/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 18.2582 - mse: 12.9396 - val_loss: 21.5251 - val_mse: 16.3726\n",
            "Epoch 10/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 17.3641 - mse: 12.3219 - val_loss: 25.7147 - val_mse: 20.7987\n",
            "Epoch 11/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 17.0328 - mse: 12.2164 - val_loss: 16.6337 - val_mse: 11.9279\n",
            "Epoch 12/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 15.1675 - mse: 10.5479 - val_loss: 21.3266 - val_mse: 16.8011\n",
            "Epoch 13/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 14.0846 - mse: 9.6140 - val_loss: 20.1622 - val_mse: 15.7624\n",
            "Epoch 14/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 14.0496 - mse: 9.6766 - val_loss: 22.5661 - val_mse: 18.2496\n",
            "Epoch 15/200\n",
            "21/21 [==============================] - 0s 20ms/step - loss: 13.5133 - mse: 9.2489 - val_loss: 27.2096 - val_mse: 23.0047\n",
            "Epoch 16/200\n",
            "21/21 [==============================] - 0s 20ms/step - loss: 15.7153 - mse: 11.5483 - val_loss: 19.3942 - val_mse: 15.2803\n",
            "Epoch 17/200\n",
            "21/21 [==============================] - 0s 19ms/step - loss: 13.6827 - mse: 9.6199 - val_loss: 20.7737 - val_mse: 16.7661\n",
            "Epoch 18/200\n",
            "21/21 [==============================] - 0s 20ms/step - loss: 13.4824 - mse: 9.5006 - val_loss: 16.9772 - val_mse: 13.0309\n",
            "Epoch 19/200\n",
            "21/21 [==============================] - 0s 19ms/step - loss: 12.4041 - mse: 8.5062 - val_loss: 22.6792 - val_mse: 18.8142\n",
            "Epoch 20/200\n",
            "21/21 [==============================] - 0s 15ms/step - loss: 15.6209 - mse: 11.7914 - val_loss: 14.6452 - val_mse: 10.8531\n",
            "Epoch 21/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 13.1623 - mse: 9.4117 - val_loss: 17.3661 - val_mse: 13.6616\n",
            "Epoch 22/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 11.4146 - mse: 7.7445 - val_loss: 15.5535 - val_mse: 11.9224\n",
            "Epoch 23/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 10.8819 - mse: 7.2787 - val_loss: 16.6424 - val_mse: 13.0632\n",
            "Epoch 24/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 10.7033 - mse: 7.1438 - val_loss: 17.8883 - val_mse: 14.3630\n",
            "Epoch 25/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 10.5765 - mse: 7.0834 - val_loss: 22.2135 - val_mse: 18.7490\n",
            "Epoch 26/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 13.6008 - mse: 10.1604 - val_loss: 20.0992 - val_mse: 16.6762\n",
            "Epoch 27/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 10.0892 - mse: 6.6967 - val_loss: 12.8963 - val_mse: 9.5422\n",
            "Epoch 28/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 11.5611 - mse: 8.2220 - val_loss: 17.2222 - val_mse: 13.8984\n",
            "Epoch 29/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 15.0699 - mse: 11.7275 - val_loss: 15.8743 - val_mse: 12.5609\n",
            "Epoch 30/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 15.8382 - mse: 12.5367 - val_loss: 18.9746 - val_mse: 15.7043\n",
            "Epoch 31/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 10.5858 - mse: 7.3425 - val_loss: 18.2590 - val_mse: 15.0480\n",
            "Epoch 32/200\n",
            "21/21 [==============================] - 0s 15ms/step - loss: 10.3188 - mse: 7.1315 - val_loss: 13.8978 - val_mse: 10.7395\n",
            "Epoch 33/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 9.5593 - mse: 6.4253 - val_loss: 13.9827 - val_mse: 10.8761\n",
            "Epoch 34/200\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 9.3872 - mse: 6.3023 - val_loss: 16.9347 - val_mse: 13.8728\n",
            "Epoch 35/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 10.3202 - mse: 7.2796 - val_loss: 14.9775 - val_mse: 11.9556\n",
            "Epoch 36/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 10.5142 - mse: 7.5134 - val_loss: 17.4577 - val_mse: 14.4770\n",
            "Epoch 37/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 8.8136 - mse: 5.8556 - val_loss: 15.4303 - val_mse: 12.4936\n",
            "Epoch 38/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 9.7283 - mse: 6.8098 - val_loss: 12.8313 - val_mse: 9.9343\n",
            "Epoch 39/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 11.0300 - mse: 8.1487 - val_loss: 17.1976 - val_mse: 14.3347\n",
            "Epoch 40/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 9.9798 - mse: 7.1334 - val_loss: 17.4775 - val_mse: 14.6485\n",
            "Epoch 41/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 9.2934 - mse: 6.4834 - val_loss: 13.7940 - val_mse: 11.0074\n",
            "Epoch 42/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 8.2179 - mse: 5.4469 - val_loss: 14.7971 - val_mse: 12.0484\n",
            "Epoch 43/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 9.2818 - mse: 6.5499 - val_loss: 14.1704 - val_mse: 11.4598\n",
            "Epoch 44/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 7.7892 - mse: 5.0955 - val_loss: 12.7644 - val_mse: 10.0900\n",
            "Epoch 45/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 9.3704 - mse: 6.7101 - val_loss: 13.6653 - val_mse: 11.0241\n",
            "Epoch 46/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 9.6163 - mse: 6.9859 - val_loss: 15.9835 - val_mse: 13.3714\n",
            "Epoch 47/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 10.1996 - mse: 7.6026 - val_loss: 15.2626 - val_mse: 12.6709\n",
            "Epoch 48/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 10.0038 - mse: 7.4358 - val_loss: 14.2466 - val_mse: 11.7022\n",
            "Epoch 49/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 9.0807 - mse: 6.5520 - val_loss: 13.7669 - val_mse: 11.2562\n",
            "Epoch 50/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 8.0702 - mse: 5.5748 - val_loss: 14.7459 - val_mse: 12.2708\n",
            "Epoch 51/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 9.2344 - mse: 6.7730 - val_loss: 12.4395 - val_mse: 9.9941\n",
            "Epoch 52/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 7.9142 - mse: 5.4831 - val_loss: 13.1800 - val_mse: 10.7647\n",
            "Epoch 53/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 7.3625 - mse: 4.9591 - val_loss: 11.9348 - val_mse: 9.5498\n",
            "Epoch 54/200\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 6.9776 - mse: 4.6045 - val_loss: 13.1993 - val_mse: 10.8423\n",
            "Epoch 55/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 7.4242 - mse: 5.0806 - val_loss: 12.4634 - val_mse: 10.1365\n",
            "Epoch 56/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 9.1780 - mse: 6.8633 - val_loss: 24.8504 - val_mse: 22.5508\n",
            "Epoch 57/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 8.9993 - mse: 6.7104 - val_loss: 10.6476 - val_mse: 8.3747\n",
            "Epoch 58/200\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 7.4696 - mse: 5.2133 - val_loss: 13.0796 - val_mse: 10.8416\n",
            "Epoch 59/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 6.3601 - mse: 4.1359 - val_loss: 12.0635 - val_mse: 9.8544\n",
            "Epoch 60/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 7.0404 - mse: 4.8403 - val_loss: 14.0591 - val_mse: 11.8725\n",
            "Epoch 61/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 7.6055 - mse: 5.4306 - val_loss: 15.1825 - val_mse: 13.0185\n",
            "Epoch 62/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 7.9296 - mse: 5.7799 - val_loss: 12.4589 - val_mse: 10.3241\n",
            "Epoch 63/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 7.4153 - mse: 5.2938 - val_loss: 21.4915 - val_mse: 19.3804\n",
            "Epoch 64/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 7.8620 - mse: 5.7682 - val_loss: 14.5218 - val_mse: 12.4436\n",
            "Epoch 65/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 7.2454 - mse: 5.1803 - val_loss: 19.0927 - val_mse: 17.0360\n",
            "Epoch 66/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 9.4316 - mse: 7.3908 - val_loss: 13.0718 - val_mse: 11.0460\n",
            "Epoch 67/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 16.5445 - mse: 14.5255 - val_loss: 29.3650 - val_mse: 27.3516\n",
            "Epoch 68/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 12.6120 - mse: 10.6031 - val_loss: 15.8150 - val_mse: 13.8202\n",
            "Epoch 69/200\n",
            "21/21 [==============================] - 0s 15ms/step - loss: 7.5639 - mse: 5.5839 - val_loss: 11.5083 - val_mse: 9.5457\n",
            "Epoch 70/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 5.9306 - mse: 3.9806 - val_loss: 10.6334 - val_mse: 8.6987\n",
            "Epoch 71/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 6.8482 - mse: 4.9224 - val_loss: 12.1185 - val_mse: 10.2039\n",
            "Epoch 72/200\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 5.8555 - mse: 3.9534 - val_loss: 12.2858 - val_mse: 10.3936\n",
            "Epoch 73/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 6.5693 - mse: 4.6909 - val_loss: 10.9169 - val_mse: 9.0539\n",
            "Epoch 74/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 6.1218 - mse: 4.2670 - val_loss: 10.6573 - val_mse: 8.8159\n",
            "Epoch 75/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 5.3333 - mse: 3.5010 - val_loss: 14.3999 - val_mse: 12.5791\n",
            "Epoch 76/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 6.3601 - mse: 4.5498 - val_loss: 11.9626 - val_mse: 10.1656\n",
            "Epoch 77/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 6.1250 - mse: 4.3353 - val_loss: 14.2590 - val_mse: 12.4825\n",
            "Epoch 78/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 7.0470 - mse: 5.2782 - val_loss: 11.2528 - val_mse: 9.4979\n",
            "Epoch 79/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 8.9766 - mse: 7.2266 - val_loss: 9.0305 - val_mse: 7.2942\n",
            "Epoch 80/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 6.4975 - mse: 4.7732 - val_loss: 11.7492 - val_mse: 10.0362\n",
            "Epoch 81/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 5.7807 - mse: 4.0777 - val_loss: 10.7420 - val_mse: 9.0503\n",
            "Epoch 82/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 5.9983 - mse: 4.3163 - val_loss: 11.5504 - val_mse: 9.8776\n",
            "Epoch 83/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 6.1580 - mse: 4.4907 - val_loss: 11.1518 - val_mse: 9.4963\n",
            "Epoch 84/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 6.7975 - mse: 5.1530 - val_loss: 13.7080 - val_mse: 12.0734\n",
            "Epoch 85/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 11.3379 - mse: 9.7133 - val_loss: 14.3102 - val_mse: 12.6947\n",
            "Epoch 86/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 7.6135 - mse: 6.0066 - val_loss: 9.1427 - val_mse: 7.5484\n",
            "Epoch 87/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 6.8853 - mse: 5.3003 - val_loss: 11.8440 - val_mse: 10.2684\n",
            "Epoch 88/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 6.3257 - mse: 4.7586 - val_loss: 10.1801 - val_mse: 8.6254\n",
            "Epoch 89/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 5.1815 - mse: 3.6355 - val_loss: 11.3079 - val_mse: 9.7692\n",
            "Epoch 90/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 6.0156 - mse: 4.4863 - val_loss: 10.8157 - val_mse: 9.2954\n",
            "Epoch 91/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 6.1096 - mse: 4.5971 - val_loss: 11.4516 - val_mse: 9.9523\n",
            "Epoch 92/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 6.3683 - mse: 4.8732 - val_loss: 10.9349 - val_mse: 9.4505\n",
            "Epoch 93/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 7.9094 - mse: 6.4309 - val_loss: 12.7383 - val_mse: 11.2729\n",
            "Epoch 94/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 6.3131 - mse: 4.8554 - val_loss: 11.2850 - val_mse: 9.8381\n",
            "Epoch 95/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 5.7729 - mse: 4.3336 - val_loss: 13.5436 - val_mse: 12.1138\n",
            "Epoch 96/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 5.0843 - mse: 3.6556 - val_loss: 15.0002 - val_mse: 13.5834\n",
            "Epoch 97/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 5.3947 - mse: 3.9864 - val_loss: 10.7376 - val_mse: 9.3418\n",
            "Epoch 98/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 5.9355 - mse: 4.5460 - val_loss: 12.0244 - val_mse: 10.6433\n",
            "Epoch 99/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 6.0792 - mse: 4.7043 - val_loss: 12.3143 - val_mse: 10.9492\n",
            "Epoch 100/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 5.1216 - mse: 3.7633 - val_loss: 17.6310 - val_mse: 16.2688\n",
            "Epoch 101/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 6.0727 - mse: 4.7218 - val_loss: 13.6193 - val_mse: 12.2752\n",
            "Epoch 102/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 5.5985 - mse: 4.2674 - val_loss: 12.2180 - val_mse: 10.9017\n",
            "Epoch 103/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 4.8252 - mse: 3.5113 - val_loss: 11.3522 - val_mse: 10.0433\n",
            "Epoch 104/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 5.5946 - mse: 4.2929 - val_loss: 10.1467 - val_mse: 8.8557\n",
            "Epoch 105/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 5.0120 - mse: 3.7288 - val_loss: 8.9747 - val_mse: 7.7015\n",
            "Epoch 106/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 5.5079 - mse: 4.2370 - val_loss: 13.3470 - val_mse: 12.0854\n",
            "Epoch 107/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 5.6394 - mse: 4.3841 - val_loss: 12.7252 - val_mse: 11.4768\n",
            "Epoch 108/200\n",
            "21/21 [==============================] - 1s 27ms/step - loss: 5.4837 - mse: 4.2423 - val_loss: 9.3993 - val_mse: 8.1534\n",
            "Epoch 109/200\n",
            "21/21 [==============================] - 0s 21ms/step - loss: 5.8677 - mse: 4.6144 - val_loss: 14.4364 - val_mse: 13.1819\n",
            "Epoch 110/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 4.7524 - mse: 3.5148 - val_loss: 12.3034 - val_mse: 11.0827\n",
            "Epoch 111/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 5.2102 - mse: 3.9967 - val_loss: 12.7170 - val_mse: 11.5066\n",
            "Epoch 112/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 5.7940 - mse: 4.5988 - val_loss: 15.6579 - val_mse: 14.4629\n",
            "Epoch 113/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 6.8896 - mse: 5.7106 - val_loss: 13.8558 - val_mse: 12.6865\n",
            "Epoch 114/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 6.1267 - mse: 4.9637 - val_loss: 15.3178 - val_mse: 14.1467\n",
            "Epoch 115/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 7.1116 - mse: 5.9492 - val_loss: 13.2896 - val_mse: 12.1414\n",
            "Epoch 116/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 12.1570 - mse: 11.0098 - val_loss: 18.7195 - val_mse: 17.5769\n",
            "Epoch 117/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 7.0358 - mse: 5.9047 - val_loss: 15.3286 - val_mse: 14.2078\n",
            "Epoch 118/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 9.8764 - mse: 8.7609 - val_loss: 12.6098 - val_mse: 11.5014\n",
            "Epoch 119/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 7.4215 - mse: 6.3212 - val_loss: 9.9165 - val_mse: 8.8258\n",
            "Epoch 120/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 4.9855 - mse: 3.8979 - val_loss: 11.9730 - val_mse: 10.8980\n",
            "Epoch 121/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 5.2661 - mse: 4.1949 - val_loss: 11.4865 - val_mse: 10.4226\n",
            "Epoch 122/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 4.5057 - mse: 3.4483 - val_loss: 10.9071 - val_mse: 9.8560\n",
            "Epoch 123/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 4.2400 - mse: 3.1872 - val_loss: 11.3502 - val_mse: 10.3068\n",
            "Epoch 124/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 7.6414 - mse: 6.5985 - val_loss: 14.7692 - val_mse: 13.7196\n",
            "Epoch 125/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 5.6464 - mse: 4.5944 - val_loss: 12.6360 - val_mse: 11.5860\n",
            "Epoch 126/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 4.6119 - mse: 3.5714 - val_loss: 13.0194 - val_mse: 11.9882\n",
            "Epoch 127/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 4.9225 - mse: 3.8971 - val_loss: 11.0785 - val_mse: 10.0542\n",
            "Epoch 128/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 4.5700 - mse: 3.5546 - val_loss: 13.8094 - val_mse: 12.8051\n",
            "Epoch 129/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 4.7567 - mse: 3.7565 - val_loss: 10.5631 - val_mse: 9.5721\n",
            "Epoch 130/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 5.2527 - mse: 4.2712 - val_loss: 12.1770 - val_mse: 11.1967\n",
            "Epoch 131/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 4.4285 - mse: 3.4559 - val_loss: 9.1200 - val_mse: 8.1472\n",
            "Epoch 132/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 5.0574 - mse: 4.0938 - val_loss: 11.4805 - val_mse: 10.5301\n",
            "Epoch 133/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 4.6833 - mse: 3.7327 - val_loss: 10.8498 - val_mse: 9.8984\n",
            "Epoch 134/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 4.6491 - mse: 3.7071 - val_loss: 10.5959 - val_mse: 9.6601\n",
            "Epoch 135/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 4.2946 - mse: 3.3643 - val_loss: 11.1728 - val_mse: 10.2549\n",
            "Epoch 136/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 4.4168 - mse: 3.5016 - val_loss: 11.1820 - val_mse: 10.2735\n",
            "Epoch 137/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 5.6750 - mse: 4.7670 - val_loss: 12.8910 - val_mse: 11.9892\n",
            "Epoch 138/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 5.7388 - mse: 4.8418 - val_loss: 11.4851 - val_mse: 10.5984\n",
            "Epoch 139/200\n",
            "21/21 [==============================] - 1s 29ms/step - loss: 4.7133 - mse: 3.8226 - val_loss: 15.7069 - val_mse: 14.8202\n",
            "Epoch 140/200\n",
            "21/21 [==============================] - 0s 15ms/step - loss: 6.6776 - mse: 5.7985 - val_loss: 13.4569 - val_mse: 12.5838\n",
            "Epoch 141/200\n",
            "21/21 [==============================] - 0s 17ms/step - loss: 4.3506 - mse: 3.4840 - val_loss: 8.9126 - val_mse: 8.0505\n",
            "Epoch 142/200\n",
            "21/21 [==============================] - 1s 28ms/step - loss: 4.1954 - mse: 3.3364 - val_loss: 12.0674 - val_mse: 11.2155\n",
            "Epoch 143/200\n",
            "21/21 [==============================] - 0s 15ms/step - loss: 5.7804 - mse: 4.9255 - val_loss: 14.7302 - val_mse: 13.8815\n",
            "Epoch 144/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 4.9749 - mse: 4.1254 - val_loss: 9.7858 - val_mse: 8.9392\n",
            "Epoch 145/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 5.4168 - mse: 4.5788 - val_loss: 13.4304 - val_mse: 12.6001\n",
            "Epoch 146/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 4.7022 - mse: 3.8610 - val_loss: 11.4662 - val_mse: 10.6196\n",
            "Epoch 147/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 4.5372 - mse: 3.6991 - val_loss: 10.5546 - val_mse: 9.7250\n",
            "Epoch 148/200\n",
            "21/21 [==============================] - 0s 15ms/step - loss: 4.5389 - mse: 3.7193 - val_loss: 12.1789 - val_mse: 11.3657\n",
            "Epoch 149/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 5.6179 - mse: 4.7896 - val_loss: 9.7179 - val_mse: 8.8824\n",
            "Epoch 150/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 4.6387 - mse: 3.8200 - val_loss: 17.1559 - val_mse: 16.3458\n",
            "Epoch 151/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 4.7206 - mse: 3.9200 - val_loss: 11.3773 - val_mse: 10.5878\n",
            "Epoch 152/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 4.3545 - mse: 3.5710 - val_loss: 10.4349 - val_mse: 9.6498\n",
            "Epoch 153/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 4.1418 - mse: 3.3597 - val_loss: 10.2204 - val_mse: 9.4415\n",
            "Epoch 154/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 3.7248 - mse: 2.9542 - val_loss: 10.4574 - val_mse: 9.6968\n",
            "Epoch 155/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 3.6648 - mse: 2.9077 - val_loss: 11.7323 - val_mse: 10.9827\n",
            "Epoch 156/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 4.4109 - mse: 3.6620 - val_loss: 11.6653 - val_mse: 10.9215\n",
            "Epoch 157/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 4.9334 - mse: 4.1909 - val_loss: 12.1307 - val_mse: 11.3942\n",
            "Epoch 158/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 4.1450 - mse: 3.4088 - val_loss: 12.1316 - val_mse: 11.4023\n",
            "Epoch 159/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 3.9380 - mse: 3.2085 - val_loss: 10.9414 - val_mse: 10.2172\n",
            "Epoch 160/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 3.6462 - mse: 2.9241 - val_loss: 12.6388 - val_mse: 11.9151\n",
            "Epoch 161/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 3.8510 - mse: 3.1322 - val_loss: 9.1777 - val_mse: 8.4665\n",
            "Epoch 162/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 3.5788 - mse: 2.8710 - val_loss: 11.0974 - val_mse: 10.3952\n",
            "Epoch 163/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 3.1927 - mse: 2.4840 - val_loss: 10.8528 - val_mse: 10.1487\n",
            "Epoch 164/200\n",
            "21/21 [==============================] - 0s 16ms/step - loss: 3.5169 - mse: 2.8177 - val_loss: 10.4486 - val_mse: 9.7544\n",
            "Epoch 165/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 3.9985 - mse: 3.3077 - val_loss: 12.0498 - val_mse: 11.3588\n",
            "Epoch 166/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 4.5614 - mse: 3.8747 - val_loss: 10.0619 - val_mse: 9.3809\n",
            "Epoch 167/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 4.1048 - mse: 3.4229 - val_loss: 12.2576 - val_mse: 11.5624\n",
            "Epoch 168/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 4.0058 - mse: 3.3251 - val_loss: 11.0800 - val_mse: 10.4010\n",
            "Epoch 169/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 3.8173 - mse: 3.1429 - val_loss: 15.6544 - val_mse: 14.9763\n",
            "Epoch 170/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 4.2436 - mse: 3.5729 - val_loss: 9.5098 - val_mse: 8.8486\n",
            "Epoch 171/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 3.9732 - mse: 3.3160 - val_loss: 11.0613 - val_mse: 10.4111\n",
            "Epoch 172/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 4.7232 - mse: 4.0669 - val_loss: 13.4792 - val_mse: 12.8302\n",
            "Epoch 173/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 4.0712 - mse: 3.4237 - val_loss: 10.2286 - val_mse: 9.5875\n",
            "Epoch 174/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 3.7277 - mse: 3.0869 - val_loss: 11.8132 - val_mse: 11.1720\n",
            "Epoch 175/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 3.2295 - mse: 2.5881 - val_loss: 12.2076 - val_mse: 11.5649\n",
            "Epoch 176/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 3.7142 - mse: 3.0787 - val_loss: 10.8020 - val_mse: 10.1672\n",
            "Epoch 177/200\n",
            "21/21 [==============================] - 0s 15ms/step - loss: 3.3015 - mse: 2.6753 - val_loss: 10.5228 - val_mse: 9.9047\n",
            "Epoch 178/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 3.4275 - mse: 2.8099 - val_loss: 12.8124 - val_mse: 12.1962\n",
            "Epoch 179/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 3.5826 - mse: 2.9700 - val_loss: 10.7133 - val_mse: 10.1061\n",
            "Epoch 180/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 3.3420 - mse: 2.7308 - val_loss: 13.8991 - val_mse: 13.2862\n",
            "Epoch 181/200\n",
            "21/21 [==============================] - 0s 15ms/step - loss: 3.3364 - mse: 2.7326 - val_loss: 12.3677 - val_mse: 11.7688\n",
            "Epoch 182/200\n",
            "21/21 [==============================] - 0s 16ms/step - loss: 3.1927 - mse: 2.5979 - val_loss: 9.8302 - val_mse: 9.2367\n",
            "Epoch 183/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 3.1064 - mse: 2.5148 - val_loss: 10.7850 - val_mse: 10.1873\n",
            "Epoch 184/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 4.0360 - mse: 3.4456 - val_loss: 11.9168 - val_mse: 11.3271\n",
            "Epoch 185/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 4.3979 - mse: 3.8155 - val_loss: 12.0891 - val_mse: 11.5078\n",
            "Epoch 186/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 3.4078 - mse: 2.8278 - val_loss: 13.6802 - val_mse: 13.1013\n",
            "Epoch 187/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 4.5618 - mse: 3.9862 - val_loss: 11.5065 - val_mse: 10.9350\n",
            "Epoch 188/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 4.3072 - mse: 3.7364 - val_loss: 12.4913 - val_mse: 11.9209\n",
            "Epoch 189/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 5.5366 - mse: 4.9301 - val_loss: 11.4130 - val_mse: 10.7822\n",
            "Epoch 190/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 7.0644 - mse: 6.4609 - val_loss: 17.4669 - val_mse: 16.8930\n",
            "Epoch 191/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 6.3476 - mse: 5.7754 - val_loss: 11.4096 - val_mse: 10.8432\n",
            "Epoch 192/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 6.8183 - mse: 6.2546 - val_loss: 12.8253 - val_mse: 12.2701\n",
            "Epoch 193/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 9.1080 - mse: 8.5486 - val_loss: 26.5724 - val_mse: 26.0060\n",
            "Epoch 194/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 5.8116 - mse: 5.2537 - val_loss: 10.5686 - val_mse: 10.0188\n",
            "Epoch 195/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 3.6952 - mse: 3.1493 - val_loss: 11.8390 - val_mse: 11.2952\n",
            "Epoch 196/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 3.2372 - mse: 2.6955 - val_loss: 11.3773 - val_mse: 10.8416\n",
            "Epoch 197/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 3.6753 - mse: 3.1370 - val_loss: 13.5935 - val_mse: 13.0514\n",
            "Epoch 198/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 4.8003 - mse: 4.2656 - val_loss: 11.4713 - val_mse: 10.9371\n",
            "Epoch 199/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 4.6841 - mse: 4.1523 - val_loss: 10.8752 - val_mse: 10.3496\n",
            "Epoch 200/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 3.6644 - mse: 3.1346 - val_loss: 11.8571 - val_mse: 11.3362\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fcaf18ca090>"
            ]
          },
          "metadata": {},
          "execution_count": 422
        }
      ],
      "source": [
        "# No modifique el código\n",
        "model.fit(x_train,\n",
        "          y_train,\n",
        "          epochs=200,\n",
        "          batch_size=batch_size,\n",
        "          validation_split=0.2,\n",
        "          verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 423,
      "id": "friendly-powell",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "friendly-powell",
        "outputId": "2620b577-dbeb-4bd9-9614-6c7333c66d50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 4ms/step - loss: 16.5541 - mse: 16.0332\n",
            "Test Loss: [16.554136276245117, 16.03319549560547]\n"
          ]
        }
      ],
      "source": [
        "# No modifique el código\n",
        "results = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test Loss: {}'.format(results))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "british-vegetation",
      "metadata": {
        "id": "british-vegetation"
      },
      "source": [
        "<a name='1.3'></a>\n",
        "## Cuestión 3: Utilice el mismo modelo de la cuestión anterior pero añadiendo un callback de early stopping. Obtenga un test loss inferior al del modelo anterior"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 441,
      "id": "precise-finish",
      "metadata": {
        "id": "precise-finish"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "# Código aquí\n",
        "\n",
        "\n",
        "tf.keras.regularizers.l1(l1=0.01)\n",
        "\n",
        "model.add(layers.Dense(1024,input_shape=(13,),activation='relu'))\n",
        "model.add(layers.Dropout(0.3))\n",
        "model.add(layers.Dense(512,activation='relu'))\n",
        "model.add(layers.Dense(512,activation='relu'))\n",
        "model.add(layers.Dense(64,activation='relu',kernel_regularizer='l1'))\n",
        "model.add(layers.Dense(1,activation='relu'))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 442,
      "id": "blond-telephone",
      "metadata": {
        "id": "blond-telephone"
      },
      "outputs": [],
      "source": [
        "# Compilación del modelo\n",
        "# Código aquí\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mse'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 443,
      "id": "subsequent-roads",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "subsequent-roads",
        "outputId": "2845050e-3a32-4b7c-91b3-40b6bfe10d7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "21/21 [==============================] - 1s 21ms/step - loss: 235.1105 - mse: 219.4218 - val_loss: 80.7759 - val_mse: 66.5147\n",
            "Epoch 2/200\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 48.6913 - mse: 35.6059 - val_loss: 29.6685 - val_mse: 17.9290\n",
            "Epoch 3/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 29.0396 - mse: 18.3169 - val_loss: 25.8794 - val_mse: 16.2750\n",
            "Epoch 4/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 23.5516 - mse: 14.7387 - val_loss: 22.1527 - val_mse: 14.1759\n",
            "Epoch 5/200\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 22.0404 - mse: 14.5767 - val_loss: 20.8201 - val_mse: 13.8600\n",
            "Epoch 6/200\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 19.8359 - mse: 13.1872 - val_loss: 19.9348 - val_mse: 13.6163\n",
            "Epoch 7/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 18.0936 - mse: 12.0226 - val_loss: 18.9180 - val_mse: 13.1103\n",
            "Epoch 8/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 18.5068 - mse: 12.8900 - val_loss: 18.9896 - val_mse: 13.5862\n",
            "Epoch 9/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 15.6343 - mse: 10.3820 - val_loss: 23.5062 - val_mse: 18.4216\n",
            "Epoch 10/200\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 20.6167 - mse: 15.6502 - val_loss: 20.5135 - val_mse: 15.6776\n",
            "Epoch 11/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 18.4093 - mse: 13.6477 - val_loss: 16.7826 - val_mse: 12.0999\n",
            "Epoch 12/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 16.7952 - mse: 12.1929 - val_loss: 19.5601 - val_mse: 15.0474\n",
            "Epoch 13/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 17.0227 - mse: 12.5700 - val_loss: 23.0806 - val_mse: 18.6848\n",
            "Epoch 14/200\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 15.8764 - mse: 11.5272 - val_loss: 19.5098 - val_mse: 15.2299\n",
            "Epoch 15/200\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 16.3110 - mse: 12.0753 - val_loss: 26.9507 - val_mse: 22.7740\n",
            "Epoch 16/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 20.5092 - mse: 16.3884 - val_loss: 19.7823 - val_mse: 15.7299\n",
            "Epoch 17/200\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 12.7286 - mse: 8.7245 - val_loss: 19.7689 - val_mse: 15.8182\n",
            "Epoch 18/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 14.6455 - mse: 10.7362 - val_loss: 19.5542 - val_mse: 15.6851\n",
            "Epoch 19/200\n",
            "21/21 [==============================] - 0s 15ms/step - loss: 13.8113 - mse: 9.9691 - val_loss: 17.3733 - val_mse: 13.5668\n",
            "Epoch 20/200\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 11.6303 - mse: 7.8549 - val_loss: 17.7549 - val_mse: 14.0191\n",
            "Epoch 21/200\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 11.3920 - mse: 7.6917 - val_loss: 30.8723 - val_mse: 27.2238\n",
            "Epoch 22/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 16.9498 - mse: 13.3281 - val_loss: 21.5949 - val_mse: 18.0102\n",
            "Epoch 23/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 11.5989 - mse: 8.0538 - val_loss: 14.2237 - val_mse: 10.7231\n",
            "Epoch 24/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 10.5434 - mse: 7.0672 - val_loss: 18.3003 - val_mse: 14.8522\n",
            "Epoch 25/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 10.4958 - mse: 7.0787 - val_loss: 16.1349 - val_mse: 12.7547\n",
            "Epoch 26/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 12.2969 - mse: 8.9267 - val_loss: 26.0406 - val_mse: 22.6803\n",
            "Epoch 27/200\n",
            "21/21 [==============================] - 0s 15ms/step - loss: 18.1313 - mse: 14.7709 - val_loss: 16.4103 - val_mse: 13.0665\n",
            "Epoch 28/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 13.8358 - mse: 10.5223 - val_loss: 15.1023 - val_mse: 11.8258\n",
            "Epoch 29/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 11.1009 - mse: 7.8518 - val_loss: 16.2420 - val_mse: 13.0174\n",
            "Epoch 30/200\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 11.1710 - mse: 7.9701 - val_loss: 14.0195 - val_mse: 10.8442\n",
            "Epoch 31/200\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 11.1170 - mse: 7.9681 - val_loss: 16.2334 - val_mse: 13.1201\n",
            "Epoch 32/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 10.4977 - mse: 7.4130 - val_loss: 13.3136 - val_mse: 10.2461\n",
            "Epoch 33/200\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 9.9849 - mse: 6.9421 - val_loss: 20.2027 - val_mse: 17.1899\n",
            "Epoch 34/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 12.4610 - mse: 9.4772 - val_loss: 13.7948 - val_mse: 10.8420\n",
            "Epoch 35/200\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 9.4445 - mse: 6.5149 - val_loss: 12.3248 - val_mse: 9.4230\n",
            "Epoch 36/200\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 10.7081 - mse: 7.8274 - val_loss: 14.4205 - val_mse: 11.5481\n",
            "Epoch 37/200\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 11.7672 - mse: 8.8945 - val_loss: 15.9147 - val_mse: 13.0584\n",
            "Epoch 38/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 9.1751 - mse: 6.3365 - val_loss: 12.4722 - val_mse: 9.6627\n",
            "Epoch 39/200\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 8.5685 - mse: 5.7831 - val_loss: 13.5763 - val_mse: 10.8169\n",
            "Epoch 40/200\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 10.2156 - mse: 7.4780 - val_loss: 15.3498 - val_mse: 12.6377\n",
            "Epoch 41/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 8.6812 - mse: 5.9886 - val_loss: 16.6610 - val_mse: 13.9904\n",
            "Epoch 42/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 8.8725 - mse: 6.2237 - val_loss: 18.0139 - val_mse: 15.3877\n",
            "Epoch 43/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 8.3519 - mse: 5.7478 - val_loss: 14.7230 - val_mse: 12.1411\n",
            "Epoch 44/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 7.8561 - mse: 5.2915 - val_loss: 11.7114 - val_mse: 9.1660\n",
            "Epoch 45/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 8.6422 - mse: 6.1144 - val_loss: 15.3606 - val_mse: 12.8575\n",
            "Epoch 46/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 9.2905 - mse: 6.8078 - val_loss: 12.8996 - val_mse: 10.4380\n",
            "Epoch 47/200\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 8.7413 - mse: 6.2966 - val_loss: 13.9505 - val_mse: 11.5265\n",
            "Epoch 48/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 8.2940 - mse: 5.8856 - val_loss: 13.0955 - val_mse: 10.7009\n",
            "Epoch 49/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 7.9151 - mse: 5.5417 - val_loss: 12.7201 - val_mse: 10.3673\n",
            "Epoch 50/200\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 7.9091 - mse: 5.5717 - val_loss: 17.5523 - val_mse: 15.2302\n",
            "Epoch 51/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 8.9022 - mse: 6.5982 - val_loss: 15.3214 - val_mse: 13.0351\n",
            "Epoch 52/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 8.6212 - mse: 6.3524 - val_loss: 15.0704 - val_mse: 12.8169\n",
            "Epoch 53/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 8.2370 - mse: 5.9951 - val_loss: 11.8344 - val_mse: 9.6070\n",
            "Epoch 54/200\n",
            "21/21 [==============================] - 0s 15ms/step - loss: 7.6739 - mse: 5.4608 - val_loss: 14.2432 - val_mse: 12.0499\n",
            "Epoch 55/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 7.2148 - mse: 5.0371 - val_loss: 14.3364 - val_mse: 12.1748\n",
            "Epoch 56/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 10.9885 - mse: 8.8366 - val_loss: 9.7670 - val_mse: 7.6141\n",
            "Epoch 57/200\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 8.5784 - mse: 6.4392 - val_loss: 14.4138 - val_mse: 12.3005\n",
            "Epoch 58/200\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 7.4095 - mse: 5.3175 - val_loss: 11.7847 - val_mse: 9.7154\n",
            "Epoch 59/200\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 7.0434 - mse: 4.9900 - val_loss: 10.2481 - val_mse: 8.2132\n",
            "Epoch 60/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 7.0831 - mse: 5.0618 - val_loss: 12.0312 - val_mse: 10.0269\n",
            "Epoch 61/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 8.4239 - mse: 6.4283 - val_loss: 13.8220 - val_mse: 11.8349\n",
            "Epoch 62/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 11.6251 - mse: 9.6522 - val_loss: 25.3472 - val_mse: 23.3910\n",
            "Epoch 63/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 15.2896 - mse: 13.3447 - val_loss: 24.9985 - val_mse: 23.0678\n",
            "Epoch 64/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 8.1662 - mse: 6.2480 - val_loss: 14.2760 - val_mse: 12.3724\n",
            "Epoch 65/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 11.0933 - mse: 9.1994 - val_loss: 12.3001 - val_mse: 10.4236\n",
            "Epoch 66/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 8.7796 - mse: 6.9026 - val_loss: 10.3802 - val_mse: 8.5173\n",
            "Epoch 67/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 7.6695 - mse: 5.8209 - val_loss: 10.4363 - val_mse: 8.6048\n",
            "Epoch 68/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 6.6687 - mse: 4.8488 - val_loss: 13.9098 - val_mse: 12.1056\n",
            "Epoch 69/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 6.8141 - mse: 5.0203 - val_loss: 11.9150 - val_mse: 10.1338\n",
            "Epoch 70/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 6.9560 - mse: 5.1846 - val_loss: 13.6151 - val_mse: 11.8584\n",
            "Epoch 71/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 6.9342 - mse: 5.1887 - val_loss: 16.7738 - val_mse: 15.0387\n",
            "Epoch 72/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 6.6948 - mse: 4.9704 - val_loss: 10.8733 - val_mse: 9.1616\n",
            "Epoch 73/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 6.8806 - mse: 5.1788 - val_loss: 11.8446 - val_mse: 10.1564\n",
            "Epoch 74/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 6.9217 - mse: 5.2417 - val_loss: 13.2856 - val_mse: 11.6169\n",
            "Epoch 75/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 6.3527 - mse: 4.6946 - val_loss: 12.2547 - val_mse: 10.6049\n",
            "Epoch 76/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 6.6086 - mse: 4.9714 - val_loss: 13.6289 - val_mse: 12.0024\n",
            "Epoch 77/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 7.0223 - mse: 5.4053 - val_loss: 11.1617 - val_mse: 9.5576\n",
            "Epoch 78/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 7.0481 - mse: 5.4534 - val_loss: 21.6700 - val_mse: 20.0786\n",
            "Epoch 79/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 5.9907 - mse: 4.4136 - val_loss: 11.3407 - val_mse: 9.7769\n",
            "Epoch 80/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 6.7989 - mse: 5.2443 - val_loss: 11.5728 - val_mse: 10.0292\n",
            "Epoch 81/200\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 5.8625 - mse: 4.3292 - val_loss: 12.0345 - val_mse: 10.5136\n",
            "Epoch 82/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 6.2247 - mse: 4.7107 - val_loss: 9.6565 - val_mse: 8.1545\n",
            "Epoch 83/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 6.1933 - mse: 4.6995 - val_loss: 13.0484 - val_mse: 11.5657\n",
            "Epoch 84/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 8.3997 - mse: 6.9216 - val_loss: 12.7822 - val_mse: 11.3093\n",
            "Epoch 85/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 8.8938 - mse: 7.4325 - val_loss: 19.0716 - val_mse: 17.6172\n",
            "Epoch 86/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 8.7771 - mse: 7.3365 - val_loss: 12.0206 - val_mse: 10.5909\n",
            "Epoch 87/200\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 6.1372 - mse: 4.7178 - val_loss: 13.0556 - val_mse: 11.6450\n",
            "Epoch 88/200\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 6.4287 - mse: 5.0293 - val_loss: 14.9730 - val_mse: 13.5795\n",
            "Epoch 89/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 5.9157 - mse: 4.5316 - val_loss: 13.3269 - val_mse: 11.9530\n",
            "Epoch 90/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 5.9855 - mse: 4.6209 - val_loss: 12.6669 - val_mse: 11.3093\n",
            "Epoch 91/200\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 4.7989 - mse: 3.4490 - val_loss: 14.6278 - val_mse: 13.2811\n",
            "Epoch 92/200\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 6.1965 - mse: 4.8553 - val_loss: 15.3377 - val_mse: 14.0008\n",
            "Epoch 93/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 6.9889 - mse: 5.6622 - val_loss: 19.8421 - val_mse: 18.5154\n",
            "Epoch 94/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 7.0377 - mse: 5.7273 - val_loss: 15.2643 - val_mse: 13.9669\n",
            "Epoch 95/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 6.7797 - mse: 5.4887 - val_loss: 14.5075 - val_mse: 13.2246\n",
            "Epoch 96/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 6.1707 - mse: 4.8960 - val_loss: 10.2554 - val_mse: 8.9899\n",
            "Epoch 97/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 5.5707 - mse: 4.3121 - val_loss: 11.0376 - val_mse: 9.7854\n",
            "Epoch 98/200\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 5.5999 - mse: 4.3536 - val_loss: 11.8843 - val_mse: 10.6470\n",
            "Epoch 99/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 5.6244 - mse: 4.3932 - val_loss: 16.3551 - val_mse: 15.1252\n",
            "Epoch 100/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 5.3385 - mse: 4.1131 - val_loss: 10.0911 - val_mse: 8.8738\n",
            "Epoch 101/200\n",
            "21/21 [==============================] - 0s 16ms/step - loss: 5.8740 - mse: 4.6657 - val_loss: 12.0551 - val_mse: 10.8582\n",
            "Epoch 102/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 5.5377 - mse: 4.3477 - val_loss: 9.2852 - val_mse: 8.1044\n",
            "Epoch 103/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 5.7075 - mse: 4.5310 - val_loss: 12.8226 - val_mse: 11.6517\n",
            "Epoch 104/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 5.3799 - mse: 4.2101 - val_loss: 10.5424 - val_mse: 9.3742\n",
            "Epoch 105/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 5.3659 - mse: 4.2042 - val_loss: 9.0474 - val_mse: 7.8971\n",
            "Epoch 106/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 6.4725 - mse: 5.3296 - val_loss: 12.3475 - val_mse: 11.2157\n",
            "Epoch 107/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 5.8250 - mse: 4.6995 - val_loss: 12.8075 - val_mse: 11.6882\n",
            "Epoch 108/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 5.8399 - mse: 4.7315 - val_loss: 11.8249 - val_mse: 10.7269\n",
            "Epoch 109/200\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 5.2014 - mse: 4.1083 - val_loss: 10.3416 - val_mse: 9.2575\n",
            "Epoch 110/200\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 5.6495 - mse: 4.5702 - val_loss: 11.3184 - val_mse: 10.2461\n",
            "Epoch 111/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 5.0269 - mse: 3.9606 - val_loss: 12.6317 - val_mse: 11.5725\n",
            "Epoch 112/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 5.5779 - mse: 4.5227 - val_loss: 10.2116 - val_mse: 9.1651\n",
            "Epoch 113/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 4.5679 - mse: 3.5281 - val_loss: 10.9243 - val_mse: 9.8919\n",
            "Epoch 114/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 5.0152 - mse: 3.9900 - val_loss: 9.0969 - val_mse: 8.0781\n",
            "Epoch 115/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 4.5271 - mse: 3.5142 - val_loss: 9.7014 - val_mse: 8.6918\n",
            "Epoch 116/200\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 8.5383 - mse: 7.5208 - val_loss: 13.9776 - val_mse: 12.9692\n",
            "Epoch 117/200\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 5.9549 - mse: 4.9553 - val_loss: 14.8122 - val_mse: 13.8214\n",
            "Epoch 118/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 6.0722 - mse: 5.0825 - val_loss: 12.2866 - val_mse: 11.2952\n",
            "Epoch 119/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 4.6941 - mse: 3.7081 - val_loss: 10.0370 - val_mse: 9.0585\n",
            "Epoch 120/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 5.1350 - mse: 4.1617 - val_loss: 11.8009 - val_mse: 10.8350\n",
            "Epoch 121/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 4.7443 - mse: 3.7866 - val_loss: 10.4148 - val_mse: 9.4676\n",
            "Epoch 122/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 4.9756 - mse: 4.0344 - val_loss: 11.2044 - val_mse: 10.2699\n",
            "Epoch 123/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 4.9360 - mse: 4.0079 - val_loss: 17.0355 - val_mse: 16.1058\n",
            "Epoch 124/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 6.1429 - mse: 5.2245 - val_loss: 10.2634 - val_mse: 9.3507\n",
            "Epoch 125/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 4.7355 - mse: 3.8262 - val_loss: 8.5410 - val_mse: 7.6417\n",
            "Epoch 126/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 5.1372 - mse: 4.2339 - val_loss: 13.4291 - val_mse: 12.5377\n",
            "Epoch 127/200\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 5.3081 - mse: 4.4206 - val_loss: 9.1158 - val_mse: 8.2375\n",
            "Epoch 128/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 4.1278 - mse: 3.2533 - val_loss: 9.7928 - val_mse: 8.9244\n",
            "Epoch 129/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 4.6552 - mse: 3.7900 - val_loss: 10.1102 - val_mse: 9.2503\n",
            "Epoch 130/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 5.4052 - mse: 4.5479 - val_loss: 11.9342 - val_mse: 11.0839\n",
            "Epoch 131/200\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 5.2511 - mse: 4.4022 - val_loss: 14.1960 - val_mse: 13.3524\n",
            "Epoch 132/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 5.8164 - mse: 4.9715 - val_loss: 21.4234 - val_mse: 20.5859\n",
            "Epoch 133/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 5.2435 - mse: 4.4097 - val_loss: 13.7175 - val_mse: 12.8917\n",
            "Epoch 134/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 4.2598 - mse: 3.4349 - val_loss: 10.3414 - val_mse: 9.5173\n",
            "Epoch 135/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 3.7968 - mse: 2.9818 - val_loss: 10.5781 - val_mse: 9.7676\n",
            "Epoch 136/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 5.4348 - mse: 4.6275 - val_loss: 13.8929 - val_mse: 13.0939\n",
            "Epoch 137/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 5.3616 - mse: 4.5625 - val_loss: 11.9675 - val_mse: 11.1680\n",
            "Epoch 138/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 4.4940 - mse: 3.7039 - val_loss: 10.9896 - val_mse: 10.2027\n",
            "Epoch 139/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 4.9839 - mse: 4.2033 - val_loss: 10.7407 - val_mse: 9.9693\n",
            "Epoch 140/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 4.2188 - mse: 3.4476 - val_loss: 10.6724 - val_mse: 9.9071\n",
            "Epoch 141/200\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 5.2017 - mse: 4.4392 - val_loss: 11.1050 - val_mse: 10.3494\n",
            "Epoch 142/200\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 3.8937 - mse: 3.1405 - val_loss: 11.9605 - val_mse: 11.2137\n",
            "Epoch 143/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 4.0575 - mse: 3.3110 - val_loss: 11.0238 - val_mse: 10.2813\n",
            "Epoch 144/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 4.3043 - mse: 3.5648 - val_loss: 8.9965 - val_mse: 8.2609\n",
            "Epoch 145/200\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 4.7882 - mse: 4.0565 - val_loss: 10.4379 - val_mse: 9.7079\n",
            "Epoch 146/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 3.7045 - mse: 2.9823 - val_loss: 9.4433 - val_mse: 8.7256\n",
            "Epoch 147/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 3.8470 - mse: 3.1321 - val_loss: 8.6384 - val_mse: 7.9278\n",
            "Epoch 148/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 3.4671 - mse: 2.7594 - val_loss: 10.3038 - val_mse: 9.6001\n",
            "Epoch 149/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 4.2557 - mse: 3.5543 - val_loss: 9.6633 - val_mse: 8.9660\n",
            "Epoch 150/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 5.1749 - mse: 4.4776 - val_loss: 8.5797 - val_mse: 7.8814\n",
            "Epoch 151/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 4.2826 - mse: 3.5887 - val_loss: 12.2899 - val_mse: 11.5997\n",
            "Epoch 152/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 3.8266 - mse: 3.1407 - val_loss: 10.7160 - val_mse: 10.0356\n",
            "Epoch 153/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 3.7621 - mse: 3.0859 - val_loss: 10.9743 - val_mse: 10.3041\n",
            "Epoch 154/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 3.5964 - mse: 2.9261 - val_loss: 10.0342 - val_mse: 9.3676\n",
            "Epoch 155/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 4.1152 - mse: 3.4497 - val_loss: 11.6839 - val_mse: 11.0188\n",
            "Epoch 156/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 4.0312 - mse: 3.3740 - val_loss: 10.0752 - val_mse: 9.4195\n",
            "Epoch 157/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 3.8136 - mse: 3.1601 - val_loss: 12.0307 - val_mse: 11.3779\n",
            "Epoch 158/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 4.0475 - mse: 3.3960 - val_loss: 9.9744 - val_mse: 9.3292\n",
            "Epoch 159/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 3.5508 - mse: 2.9100 - val_loss: 9.8012 - val_mse: 9.1667\n",
            "Epoch 160/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 3.6714 - mse: 3.0341 - val_loss: 11.5051 - val_mse: 10.8740\n",
            "Epoch 161/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 3.6044 - mse: 2.9723 - val_loss: 9.8673 - val_mse: 9.2302\n",
            "Epoch 162/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 3.7821 - mse: 3.1545 - val_loss: 9.6927 - val_mse: 9.0704\n",
            "Epoch 163/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 6.4562 - mse: 5.8266 - val_loss: 16.1255 - val_mse: 15.5052\n",
            "Epoch 164/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 5.1921 - mse: 4.5744 - val_loss: 18.3195 - val_mse: 17.6975\n",
            "Epoch 165/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 5.4086 - mse: 4.7937 - val_loss: 12.7287 - val_mse: 12.1162\n",
            "Epoch 166/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 4.3273 - mse: 3.7064 - val_loss: 9.8407 - val_mse: 9.2119\n",
            "Epoch 167/200\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 5.7534 - mse: 5.1374 - val_loss: 11.2659 - val_mse: 10.6571\n",
            "Epoch 168/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 4.7657 - mse: 4.1632 - val_loss: 13.0224 - val_mse: 12.4255\n",
            "Epoch 169/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 4.1040 - mse: 3.5122 - val_loss: 10.5150 - val_mse: 9.9208\n",
            "Epoch 170/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 4.7365 - mse: 4.1445 - val_loss: 10.8721 - val_mse: 10.2893\n",
            "Epoch 171/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 4.3699 - mse: 3.7865 - val_loss: 10.8588 - val_mse: 10.2750\n",
            "Epoch 172/200\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 4.5340 - mse: 3.9546 - val_loss: 19.2722 - val_mse: 18.6916\n",
            "Epoch 173/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 5.8259 - mse: 5.2511 - val_loss: 13.9822 - val_mse: 13.4105\n",
            "Epoch 174/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 6.1015 - mse: 5.5334 - val_loss: 15.4617 - val_mse: 14.8957\n",
            "Epoch 175/200\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 5.6013 - mse: 5.0336 - val_loss: 9.9110 - val_mse: 9.3465\n",
            "Epoch 175: early stopping\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fcadf268ad0>"
            ]
          },
          "metadata": {},
          "execution_count": 443
        }
      ],
      "source": [
        "## definir el early stopping callback\n",
        "# Código aquí\n",
        "es_callback = keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',  \n",
        "    patience=50, \n",
        "    verbose=1)\n",
        "\n",
        "model.fit(x_train,\n",
        "          y_train,\n",
        "          epochs=200,\n",
        "          batch_size=16,\n",
        "          validation_split=0.2,\n",
        "          verbose=1,\n",
        "          callbacks=[es_callback]) # Código aquí"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 444,
      "id": "pressing-object",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pressing-object",
        "outputId": "5e0bd7f3-b4aa-4927-dc8d-ae0e5ff7b954"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 5ms/step - loss: 15.8542 - mse: 15.2897\n",
            "Test Loss: [15.85419750213623, 15.2896728515625]\n"
          ]
        }
      ],
      "source": [
        "# No modifique el código\n",
        "results = model.evaluate(x_test, y_test, verbose=1)\n",
        "\n",
        "print('Test Loss: {}'.format(results))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "addressed-lesbian",
      "metadata": {
        "id": "addressed-lesbian"
      },
      "source": [
        "<a name='1.4'></a>\n",
        "## Cuestión 4: ¿Podría haberse usado otra función de activación de la neurona de salida? En caso afirmativo especifíquela."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ruled-silicon",
      "metadata": {
        "id": "ruled-silicon"
      },
      "source": [
        "Si, Como es un resultado de regresión positiva, podríamos haber usado tanto la activación linear como relu."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "robust-christianity",
      "metadata": {
        "id": "robust-christianity"
      },
      "source": [
        "<a name='1.5'></a>\n",
        "## Cuestión 5:  ¿Qué es lo que una neurona calcula?\n",
        "\n",
        "**a)** Una función de activación seguida de una suma ponderada  de las entradas.\n",
        "\n",
        "**b)** Una suma ponderada  de las entradas seguida de una función de activación.\n",
        "\n",
        "**c)** Una función de pérdida, definida sobre el target.\n",
        "\n",
        "**d)** Ninguna  de las anteriores es correcta\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "joined-burden",
      "metadata": {
        "id": "joined-burden"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iraqi-european",
      "metadata": {
        "id": "iraqi-european"
      },
      "source": [
        "<a name='1.6'></a>\n",
        "## Cuestión 6:  ¿Cuál de estas funciones de activación no debería usarse en una capa oculta (hidden layer)?\n",
        "\n",
        "**a)** `sigmoid`\n",
        "\n",
        "**b)** `tanh`\n",
        "\n",
        "**c)** `relu`\n",
        "\n",
        "**d)** `linear`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cardiovascular-attack",
      "metadata": {
        "id": "cardiovascular-attack"
      },
      "source": [
        "d) Linear"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ranging-utilization",
      "metadata": {
        "id": "ranging-utilization"
      },
      "source": [
        "<a name='1.7'></a>\n",
        "## Cuestión 7:  ¿Cuál de estas técnicas es efectiva para combatir el overfitting en una red con varias capas ocultas? Ponga todas las que lo sean.\n",
        "\n",
        "**a)** Dropout\n",
        "\n",
        "**b)** Regularización L2.\n",
        "\n",
        "**c)** Aumentar el tamaño del test set.\n",
        "\n",
        "**d)** Aumentar el tamaño del validation set.\n",
        "\n",
        "**e)** Reducir el número de capas de la red.\n",
        "\n",
        "**f)** Data augmentation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "accessible-trainer",
      "metadata": {
        "id": "accessible-trainer"
      },
      "source": [
        "Dropout,\n",
        "Regulariazción L2,\n",
        "Reducir el número de capas de la red,\n",
        "Data augmentation.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "recreational-deposit",
      "metadata": {
        "id": "recreational-deposit"
      },
      "source": [
        "<a name='1.8'></a>\n",
        "## Cuestión 8:  Supongamos que queremos entrenar una red para un problema de clasificación de imágenes con las siguientes clases: {'perro','gato','persona'}. ¿Cuántas neuronas y que función de activación debería tener la capa de salida? ¿Qué función de pérdida (loss function) debería usarse?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "confirmed-roulette",
      "metadata": {
        "id": "confirmed-roulette"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "painful-decade",
      "metadata": {
        "id": "painful-decade"
      },
      "source": [
        "<a name='actividad_2'></a>\n",
        "# Actividad 2: Redes Convolucionales\n",
        "\n",
        "Vamos a usar el dataset [cifar-10](https://www.cs.toronto.edu/~kriz/cifar.html), que son 60000 imágenes de 32x32 a color  con 10 clases diferentes. Para realizar mejor la práctica puede consultar [Introduction_to_CNN.ipynb](https://github.com/ezponda/intro_deep_learning/blob/main/class/CNN/Introduction_to_CNN.ipynb).\n",
        "\n",
        "\n",
        "\n",
        "**Puntuación**: \n",
        "\n",
        "- [Cuestión 1](#2.1): 1 pt\n",
        "- [Cuestión 2](#2.2): 1.5 pt\n",
        "- [Cuestión 3](#2.3): 0.5 pts\n",
        "- [Cuestión 4](#2.4): 0.5 pts\n",
        "- [Cuestión 5](#2.5): 0.5 pts\n",
        "- [Cuestión 6](#2.6): 0.5 pts\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Puede normalizar las imágenes al principio o usar la capa [Rescaling](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/Rescaling):\n",
        "\n",
        "```python\n",
        "tf.keras.layers.experimental.preprocessing.Rescaling(\n",
        "    scale, offset=0.0, name=None, **kwargs\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "incorporate-terrorist",
      "metadata": {
        "id": "incorporate-terrorist"
      },
      "outputs": [],
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "y_train = y_train.flatten()\n",
        "y_test = y_test.flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "brazilian-rhythm",
      "metadata": {
        "id": "brazilian-rhythm"
      },
      "outputs": [],
      "source": [
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(x_train[i])\n",
        "    plt.xlabel(class_names[y_train[i]])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "extreme-quantum",
      "metadata": {
        "id": "extreme-quantum"
      },
      "outputs": [],
      "source": [
        "print('x_train, y_train shapes:', x_train.shape, y_train.shape)\n",
        "print('x_test, y_test shapes:', x_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "living-philosophy",
      "metadata": {
        "id": "living-philosophy"
      },
      "source": [
        "<a name='2.1'></a>\n",
        "## Cuestión 1: Cree una red convolucional con la API funcional con al menos dos capas convolucionales y al menos dos capas de pooling. Utilize sólo [Average Pooling](https://www.tensorflow.org/api_docs/python/tf/keras/layers/AveragePooling2D) y no añada ninguna regularización."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "atmospheric-sight",
      "metadata": {
        "id": "atmospheric-sight"
      },
      "outputs": [],
      "source": [
        "inputs = tf.keras.Input(shape=..., name='input')\n",
        "# reescaling = ...\n",
        "\n",
        "# Convolution + pooling layers\n",
        "...\n",
        "\n",
        "# Flattening\n",
        "...\n",
        "\n",
        "# Fully-connected\n",
        "outputs = layers.Dense(...)\n",
        "\n",
        "model = keras.Model(inputs=..., outputs=...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "needed-arena",
      "metadata": {
        "id": "needed-arena"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pursuant-paper",
      "metadata": {
        "id": "pursuant-paper"
      },
      "outputs": [],
      "source": [
        "history = model.fit(x_train, y_train, epochs=25, batch_size=64,\n",
        "                    validation_split=0.15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "applicable-honduras",
      "metadata": {
        "id": "applicable-honduras"
      },
      "outputs": [],
      "source": [
        "results = model.evaluate(x_test, y_test, verbose=0, batch_size=1000)\n",
        "print('Test Loss: {}'.format(results[0]))\n",
        "print('Test Accuracy: {}'.format(results[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "numerous-invite",
      "metadata": {
        "id": "numerous-invite"
      },
      "source": [
        "<a name='2.2'></a>\n",
        "## Cuestión 2: Cree un modelo con la API funcional con un máximo de 2 capas convolucionales y un máximo de 2 capas de pooling. Utilize  [Max Pooling](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D) o [Average Pooling](https://www.tensorflow.org/api_docs/python/tf/keras/layers/AveragePooling2D) y  añada la regularización que quiera. Debe obtener un `Test accuracy > 0.68`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "annual-diploma",
      "metadata": {
        "id": "annual-diploma"
      },
      "outputs": [],
      "source": [
        "inputs = tf.keras.Input(shape=..., name='input')\n",
        "# reescaling = ...\n",
        "\n",
        "# Convolution + pooling layers\n",
        "...\n",
        "\n",
        "# Flattening\n",
        "...\n",
        "\n",
        "# Fully-connected\n",
        "outputs = layers.Dense(...)\n",
        "\n",
        "model = keras.Model(inputs=..., outputs=...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "indian-messaging",
      "metadata": {
        "id": "indian-messaging"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "functional-republic",
      "metadata": {
        "id": "functional-republic"
      },
      "outputs": [],
      "source": [
        "history = model.fit(x_train, y_train, epochs=..., batch_size=...,\n",
        "                    validation_split=0.15, callbacks=lbacks=[...])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "incorrect-completion",
      "metadata": {
        "id": "incorrect-completion"
      },
      "outputs": [],
      "source": [
        "results = model.evaluate(x_test, y_test, verbose=0, batch_size=1000)\n",
        "print('Test Loss: {}'.format(results[0]))\n",
        "print('Test Accuracy: {}'.format(results[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "optical-arizona",
      "metadata": {
        "id": "optical-arizona"
      },
      "source": [
        "<a name='2.3'></a>\n",
        "## Cuestión 3: Añada data augmentation al principio del modelo\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "previous-boxing",
      "metadata": {
        "id": "previous-boxing"
      },
      "outputs": [],
      "source": [
        "data_augmentation=... "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "comprehensive-directive",
      "metadata": {
        "id": "comprehensive-directive"
      },
      "outputs": [],
      "source": [
        "inputs = tf.keras.Input(shape=..., name='input')\n",
        "data_aug= ...\n",
        "\n",
        "# reescaling = ...\n",
        "\n",
        "# Convolution + pooling layers\n",
        "...\n",
        "\n",
        "# Flattening\n",
        "...\n",
        "\n",
        "# Fully-connected\n",
        "outputs = layers.Dense(...)\n",
        "\n",
        "model = keras.Model(inputs=..., outputs=...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "statutory-covering",
      "metadata": {
        "id": "statutory-covering"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "western-energy",
      "metadata": {
        "id": "western-energy"
      },
      "outputs": [],
      "source": [
        "history = model.fit(x_train, y_train, epochs=..., batch_size=...,\n",
        "                    validation_split=0.15, callbacks=lbacks=[...])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "classical-charm",
      "metadata": {
        "id": "classical-charm"
      },
      "outputs": [],
      "source": [
        "results = model.evaluate(x_test, y_test, verbose=0, batch_size=1000)\n",
        "print('Test Loss: {}'.format(results[0]))\n",
        "print('Test Accuracy: {}'.format(results[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sweet-implement",
      "metadata": {
        "id": "sweet-implement"
      },
      "source": [
        "<a name='2.4'></a>\n",
        "## Cuestión 4: Cree el mismo  modelo de manera secuencial. No es necesario compilar ni entrenar el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "auburn-lawrence",
      "metadata": {
        "id": "auburn-lawrence"
      },
      "outputs": [],
      "source": [
        "model_seq = tf.keras.models.Sequential()\n",
        "# Código aquí\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "present-consortium",
      "metadata": {
        "id": "present-consortium"
      },
      "source": [
        "<a name='2.5'></a>\n",
        "## Cuestión 5: Si tenenemos una  una imagen de entrada de 300 x 300 a color (RGB) y queremos usar una red densa. Si la primera capa oculta tiene 100 neuronas, ¿Cuántos parámetros tendrá esa capa (sin incluir el bias) ?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "novel-calcium",
      "metadata": {
        "id": "novel-calcium"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "complicated-positive",
      "metadata": {
        "id": "complicated-positive"
      },
      "source": [
        "<a name='2.6'></a>\n",
        "## Cuestión 6   Ponga  las verdaderas ventajas de las redes convolucionales respecto a las densas\n",
        "\n",
        "**a)** Reducen el número total de parámetros, reduciendo así el overfitting.\n",
        "\n",
        "**b)** Permiten utilizar una misma 'función'  en varias localizaciones de la imagen de entrada, en lugar de aprender una función diferente para cada pixel.\n",
        "\n",
        "**c)** Permiten el uso del transfer learning.\n",
        "\n",
        "**d)** Generalmente son menos profundas, lo que facilita su entrenamiento.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dirty-nirvana",
      "metadata": {
        "id": "dirty-nirvana"
      },
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "colab": {
      "name": "Eva_Zamorano.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "recreational-deposit",
        "living-philosophy",
        "numerous-invite",
        "optical-arizona",
        "sweet-implement",
        "present-consortium",
        "complicated-positive"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}